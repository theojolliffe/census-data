{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "cf777daa-fdff-45c5-bd81-5b10594e464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "from networkx_query import search_nodes, search_edges\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "e9186ce5-d80a-4b13-95fb-1497da7a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/theojolliffe/Documents/Wayback BBC/topicScoresOrder.json', 'r') as fp:\n",
    "    topicScoresOrder = json.load(fp)\n",
    "topicScoresOrder = {i: {i[0]: 1/(1+i[1]/10) for i in topicScoresOrder[i]} for i in topicScoresOrder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "f0e2f1d5-7661-4b5b-b627-dbeae732592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbcReg = pd.read_csv('/Users/theojolliffe/Downloads/bbc_regions.csv')\n",
    "bbcRegLU = {}\n",
    "for i in bbcReg.index:\n",
    "    if bbcReg['areaCode'].iloc[i] in [lad['code'] for lad in lads]:\n",
    "        bbcRegLU[bbcReg['areaCode'].iloc[i]] = bbcReg['bbc_online'].iloc[i]\n",
    "bbcRegLU['E06000053'] = 'cornwall'\n",
    "bbcRegLU['E06000060'] = 'beds_bucks_and_herts'\n",
    "bbcRegLU['E09000001'] = 'london'\n",
    "\n",
    "regions = ['cumbria', 'lancashire', 'merseyside', 'manchester', 'tees', 'tyne_and_wear', 'Humberside', 'york_and_north_yorkshire', 'leeds_and_west_yorkshire', 'lincolnshire', 'south_yorkshire', 'birmingham_and_black_country', 'coventry_and_warwickshire', 'hereford_and_worcester', 'shropshire', 'stoke_and_staffordshire', 'derbyshire', 'leicester', 'northamptonshire', 'nottingham', 'bristol', 'cornwall', 'devon', 'gloucestershire', 'somerset', 'wiltshire', 'beds_bucks_and_herts', 'cambridgeshire', 'essex', 'norfolk', 'suffolk', 'berkshire', 'dorset', 'hampshire', 'oxford', 'kent', 'london', 'surrey', 'sussex']\n",
    "bbcNames = {\n",
    "     'derby': 'derbyshire', \n",
    "     'hereford and worcester': 'hereford_and_worcester',\n",
    "     'tyne and wear': 'tyne_and_wear',\n",
    "     'northampton': 'northamptonshire',\n",
    "     'liverpool': 'merseyside',\n",
    "     'coventry and warwickshire': 'coventry_and_warwickshire',\n",
    "     'humberside': 'Humberside',\n",
    "     'sheffield and south yorkshire': 'south_yorkshire',\n",
    "     'hampshire & isle of wight': 'hampshire',\n",
    "     'stoke and staffordshire': 'stoke_and_staffordshire',\n",
    "     'york & north yorkshire': 'york_and_north_yorkshire',\n",
    "     'birmingham and black country': 'birmingham_and_black_country',\n",
    "     'beds, bucks and herts': 'beds_bucks_and_herts',\n",
    "     'leeds and west yorkshire': 'leeds_and_west_yorkshire'\n",
    "}\n",
    "for i in bbcRegLU.keys():\n",
    "    bbcRegLU[i] = bbcRegLU[i].lower()\n",
    "    if bbcRegLU[i] not in regions:\n",
    "        try:\n",
    "            bbcRegLU[i] = bbcNames[bbcRegLU[i]]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "8a4fc7f9-c133-40b6-9715-ac4f17389a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('/Users/theojolliffe/Documents/Wayback BBC/BBCRegionalTFIDF.csv')\n",
    "tfidf = tfidf.set_index('Unnamed: 0')\n",
    "for i in tfidf.columns:\n",
    "    for j in tfidf[i].index:\n",
    "        tfidf[i].loc[j] = 1+100*tfidf[i].loc[j]\n",
    "tfidfLU = {}\n",
    "for i in regions:\n",
    "    tfidfLU[i]=tfidf.loc[i].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "f2cbdd62-bf9d-482d-a9c6-67ba1f713c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFtopicLU = {\n",
    "    'health': 'health',\n",
    "    'racial': 'ethnicity',\n",
    "    'population': 'population',\n",
    "    'age': 'agemed',\n",
    "    'work': 'economic',\n",
    "    'commuter': 'travel',\n",
    "    'housing': 'tenure'\n",
    "}\n",
    "TFtopicRev = dict((TFtopicLU[x],x) for x in TFtopicLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "08f52317-ef12-42b1-b904-5aa67684a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = pd.read_csv('./csv/lists/places_2020.csv')\n",
    "areaType = pd.read_csv('/Users/theojolliffe/Documents/Census Data/censusAreaLookup.csv')\n",
    "typeLookup = {}\n",
    "for i in areaType.index:\n",
    "    typeLookup[areaType.iloc[i]['Name']]=areaType.iloc[i]['Group name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "8d1e89cf-589b-4f99-ab2a-b1fad36ecd82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E05009289\n",
      "E05009290\n",
      "E05009293\n",
      "E05009294\n",
      "E05009295\n",
      "E05009297\n",
      "E05009298\n",
      "E05009299\n",
      "E05009300\n",
      "E05009301\n",
      "E05009303\n",
      "E05009306\n",
      "E05009307\n",
      "E05009312\n",
      "E05011090\n"
     ]
    }
   ],
   "source": [
    "areas = []\n",
    "for i in options[\"code\"]:\n",
    "    try:\n",
    "        areas.append(json.load(open(f'/Users/theojolliffe/Documents/Census Data/census-data-main/json/place/{i}.json', 'rb')))\n",
    "    except FileNotFoundError:\n",
    "        print(i)\n",
    "        pass\n",
    "    \n",
    "for area in areas:\n",
    "    area['data']['agemed']['value']['change']['all'] = area['data']['agemed']['value']['2011']['all']-area['data']['agemed']['value']['2001']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "cfc395f5-e54a-4533-b6a8-9b482632b33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seperate the areas by area type\n",
    "regions = []\n",
    "for i in areas:\n",
    "    if i['type']=='rgn':\n",
    "        regions.append(i)\n",
    "        \n",
    "lads = []\n",
    "for i in areas:\n",
    "    if i['type']=='lad':\n",
    "        lads.append(i)\n",
    "        \n",
    "countries = []\n",
    "for i in areas:\n",
    "    if (i['type']=='ew')|(i['type']=='ctry'):\n",
    "        countries.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "c06daf46-f8d7-463e-9baa-448dae6405a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find each LAD local and national ranks and put in prioirity order\n",
    "for thisLad in lads:\n",
    "    \n",
    "    # Empty array will be populted with ranks for each variable\n",
    "    ranks = []\n",
    "\n",
    "    # Filter areas with same parent\n",
    "    sister_lads = []\n",
    "    lad_code = thisLad['parents'][0]['code']\n",
    "    for i in range(len(lads)):\n",
    "        if  lad_code == lads[i]['parents'][0]['code']:\n",
    "            sister_lads.append(lads[i])\n",
    "\n",
    "    # Loop through the various data variables\n",
    "    for a in thisLad['data']:\n",
    "        if a in ['population', 'density', 'agemed']:\n",
    "            b = 'value'\n",
    "        else:\n",
    "            b = 'perc'\n",
    "        # Create nested object with localised rank\n",
    "        thisLad['data'][a][b+\"_rank_local\"] = {}    \n",
    "        for c in ['2011', 'change']:\n",
    "            thisLad['data'][a][b+\"_rank_local\"][c] = {}\n",
    "            for d in thisLad['data'][a][b][c]:\n",
    "                vari = thisLad['data'][a][b][c][d]\n",
    "\n",
    "                # Create sorted list of values from sister areas\n",
    "                group_values = []\n",
    "                for lad in sister_lads:\n",
    "                    group_values.append(lad['data'][a][b][c][d])\n",
    "                    group_values = [x if (type(x) == float) | (type(x) == int) else np.nan for x in group_values]\n",
    "                group_values.sort(reverse=True)\n",
    "\n",
    "                # Find index of value of area of interest\n",
    "                varRank = group_values.index(vari) + 1\n",
    "\n",
    "                # Convert bottom half rankings into negative values\n",
    "                if varRank>len(group_values)/2:\n",
    "                    varRank = varRank-len(group_values)-1\n",
    "                    \n",
    "                natRank = thisLad['data'][a][b+\"_rank\"][c][d]\n",
    "                if natRank > 168:\n",
    "                    natRank = natRank-336-1\n",
    "\n",
    "                thisLad['data'][a][b+\"_rank_local\"][c][d] = varRank\n",
    "\n",
    "                # Append ranking data to original array\n",
    "                ranks.append({\n",
    "                    'label': a+'_'+b+'_'+c+'_'+d, \n",
    "                    'locRank': varRank, \n",
    "                    'natRank': natRank, \n",
    "                    'value': vari})\n",
    "\n",
    "    # Sort in rank order\n",
    "    ranks = sorted(ranks, key=lambda x: (abs(x['locRank']), -abs(x['value'])))\n",
    "    thisLad[\"Priorities\"] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "27663250-5802-421e-b0ab-bb731af6224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [{1: 'population', 2: 'value', 3: 'all'}, \n",
    "          {1: 'agemed', 2: 'value', 3: 'all'},\n",
    "          {1: 'health', 2: 'perc', 3: 'good'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'car_van'},\n",
    "          {1: 'agemed', 2: 'value', 3: 'all'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'employee'},\n",
    "          {1: 'health', 2: 'perc', 3: 'bad'},\n",
    "          {1: 'ethnicity', 2: 'perc', 3: 'black'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'student'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'self-employed'}]\n",
    "\n",
    "ladCodes = []\n",
    "for lad in lads:\n",
    "    ladCodes.append(lad['code'])\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(columns=\n",
    "                  ['lad', '2001',  '2011',  'change', 'natRank', 'localRank', 'topic', 'parent'])\n",
    "for j in range(len(topics)):\n",
    "    df1 = pd.DataFrame(index=ladCodes, columns=\n",
    "                      ['lad', '2001',  '2011',  'change', 'natRank', 'localRank', 'topic', 'parent'])\n",
    "    for i in range(len(lads)):\n",
    "        df1['lad'].iloc[i] = lads[i]['name']\n",
    "        df1['2001'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2001'][topics[j][3]]\n",
    "        df1['2011'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2011'][topics[j][3]]    \n",
    "        df1['change'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['change'][topics[j][3]]\n",
    "        df1['natRank'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]+\"_rank\"]['change'][topics[j][3]]    \n",
    "        df1['localRank'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]+\"_rank_local\"]['change'][topics[j][3]]    \n",
    "        df1['topic'].iloc[i] = topics[j][1]+\"_\"+topics[j][3]\n",
    "        df1['parent'].iloc[i] = lads[i]['parents'][0]['name']\n",
    "    df = pd.concat([df,df1])\n",
    "df['natRank'] = np.where(df['natRank'] > 168, df['natRank']-337,df['natRank'])\n",
    "\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    for lad in lads:\n",
    "        \n",
    "#       Find the areas that this area has overtaken\n",
    "        v2001 = lad['data'][topic[1]][topic[2]]['2001'][topic[3]]\n",
    "        v2011 = lad['data'][topic[1]][topic[2]]['2011'][topic[3]]\n",
    "        dfT = df[(df['topic']==topic[1]+\"_\"+topic[3])&(df['2001']>v2001)&(df['2011']<v2011)]\n",
    "        obje = []\n",
    "        for i in range(dfT.shape[0]):\n",
    "            obje.append(dfT.iloc[i]['lad'])\n",
    "\n",
    "        lad['data'][topic[1]][topic[2]+\"_rank\"]['overtake'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+\"_rank\"]['overtake'][topic[3]] = obje\n",
    "        \n",
    "#       Find the area immediatly above or below\n",
    "        below = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]\n",
    "        above = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]-2\n",
    "        if below<0:\n",
    "            below=below+1\n",
    "            above=above+1\n",
    "        reg = lad['parents'][0]['name']\n",
    "        name_above = df[(df['parent']==reg)&(df['topic']==topic[1]+\"_\"+topic[3])].sort_values('2011', ascending=False).iloc[above]['lad']\n",
    "        name_below = df[(df['parent']==reg)&(df['topic']==topic[1]+\"_\"+topic[3])].sort_values('2011', ascending=False).iloc[below]['lad']\n",
    "        if above>0:\n",
    "            area_above = {'name': name_above,\n",
    "                         'value': df[(df['lad']==name_above)&(df['topic']==topic[1]+\"_\"+topic[3])]['2011'].iloc[0]}\n",
    "        else:\n",
    "            area_above = 'NaN'\n",
    "        if below<len(df[(df['parent']==reg)&(df['topic']==topic[1]+\"_\"+topic[3])]):\n",
    "            area_below = {'name': name_below,\n",
    "                          'value': df[(df['lad']==name_below)&(df['topic']==topic[1]+\"_\"+topic[3])]['2011'].iloc[0]}\n",
    "        else:\n",
    "            area_below = 'NaN'\n",
    "        \n",
    "        if 'above_below' not in lad['data'][topic[1]][topic[2]+'_rank_local'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'] = {}\n",
    "\n",
    "        if topic[3] not in lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]] = {}\n",
    "        \n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]][\"above\"] = area_above\n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]][\"below\"] = area_below\n",
    "            \n",
    "        # Add top and bottom three biggest movers for each subject to every area\n",
    "        df_topic = df[(df['topic']==topic[1]+\"_\"+topic[3])&(abs(df['natRank'])<4)]\n",
    "        ob = {}\n",
    "        for index, row in df_topic.iterrows():\n",
    "            ob[row['natRank']]= {row['lad']: row['change']}\n",
    "        if 'top_bottom' not in lad['data'][topic[1]][topic[2]+'_rank'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank']['top_bottom'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+'_rank']['top_bottom'][topic[3]]=ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "321d1d0d-35c5-4af9-92ec-92a4bb085225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out priority list by subject\n",
    "for lad in lads:\n",
    "    subjectList = [\"fair\", \"rent_free\", \"shared_ownership\", \"bicycle\", \"taxi\", \"moto\", \"bus\", \"other\", \"female\", \"male\"]\n",
    "    priorities = []\n",
    "    priorities2011 = []\n",
    "    for rank in lad['Priorities']:\n",
    "        s=rank['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        if ((s[2] == \"change\") & (s[3] not in subjectList)):\n",
    "            priorities.append(rank)\n",
    "        if (s[2]!=\"change\"):\n",
    "            priorities2011.append(rank)\n",
    "    lad['pri'] = priorities\n",
    "    lad['pri2011'] = priorities2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "d64c13c1-a32d-4ace-8e97-c687ed586386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create triple data for areas of closest proximity\n",
    "geogTriples = []\n",
    "for lad in lads:\n",
    "    list1 = lad['bounds'][0]+lad['bounds'][1]\n",
    "    for lad2 in lads:\n",
    "        list2 = lad2['bounds'][0]+lad2['bounds'][1]\n",
    "        listDif = [abs(list1[i]-list2[i]) for i in [0,1,2,3]]\n",
    "        listDif.sort()\n",
    "        if (sum(listDif[:3]) < 0.3) & (lad!=lad2):\n",
    "            geogTriples.append([lad['name'], lad2['name'], (\"near\", round(sum(listDif[:3]), 2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "6b3f567a-9979-491b-bc15-eafdef912493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storiesRefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "32234a09-04df-4687-9817-9e9e7150b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storiesRefined = sorted(storiesRefined, key=lambda x: ((abs(x['locRank'])*(1/tfidfLU['manchester'][TFtopicRev[x['label'].split('_')[0]]])), \n",
    "#                                                        abs(x['natRank']), -abs(x['value'])))\n",
    "# storiesRefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "7b470023-003d-4c90-99b6-a362502b6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicScoresOrder[bbcRegLU['E09000004']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "a6ce5654-75f7-49b2-84df-20345a2d1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storiesRefined = sorted(storiesRefined, key=lambda x: ((abs(x['locRank'])*(topicScoresOrder[bbcRegLU[thisLad['code']]][x['label'].split('_')[0]])), \n",
    "#                                                        abs(x['natRank']), -abs(x['value'])))\n",
    "# storiesRefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "932bccfd-f4dd-409d-996c-3a6d80cdc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(TFtopicRev[x['label'].split('_')[0]], 1/tfidfLU['manchester'][TFtopicRev[x['label'].split('_')[0]]]) for x in storiesRefined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93115651-d60a-4913-9150-2a7e10676e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "71d1593c-e42a-40a7-97f8-0e64a03bb693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-789-b68ed174070f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthisLad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mstoriesRefined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstoriesRefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'locRank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopicScoresOrder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbcRegLU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthisLad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'natRank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-789-b68ed174070f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthisLad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mstoriesRefined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstoriesRefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'locRank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopicScoresOrder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbcRegLU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthisLad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'natRank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "for thisLad in lads:\n",
    "\n",
    "    # Find nearby area of same area type\n",
    "    nearSimilar = []\n",
    "    for i in geogTriples:\n",
    "        try:\n",
    "            if ((i[0]==thisLad['name'])&(typeLookup[i[1]]==typeLookup[thisLad['name']])):\n",
    "                nearSimilar.append([i[0], i[1], ('near_similar', i[2][1])])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(nearSimilar)==0:\n",
    "        nearSimilar = [i for i in geogTriples if i[0]==thisLad['name']]\n",
    "\n",
    "    nearSimilar = sorted(nearSimilar, key=lambda x: x[2][1])\n",
    "\n",
    "\n",
    "    # Find and refine stories\n",
    "    stories = []\n",
    "    stories=stories+[i for i in thisLad['pri'] if abs(i['locRank']) <= 5]\n",
    "    stories=stories+[i for i in thisLad['pri'] if (abs(i['natRank']) <= 10)&(abs(i['locRank']) > 5)]\n",
    "\n",
    "    change = sorted(thisLad['pri'], reverse=True, key=lambda x: abs(x['value']))\n",
    "\n",
    "    bigchange = [i for i in change if abs(i['value'])>4.5]\n",
    "    for i in range(len(bigchange)):\n",
    "        if len(stories)<4:\n",
    "            stories=stories+[bigchange[i]]\n",
    "\n",
    "    smallchange = sorted([i for i in change if abs(i['value'])<1], reverse=False, key=lambda x: abs(x['value']))\n",
    "    for i in range(len(smallchange)):\n",
    "        if len(stories)<5:\n",
    "            stories=stories+[smallchange[i]]\n",
    "\n",
    "    stories = sorted(stories, key=lambda x: (abs(x['locRank']), abs(x['natRank']), -abs(x['value'])))\n",
    "    stories = sorted(stories, reverse=True, key=lambda x: x['label']=='population_value_change_all')\n",
    "\n",
    "    storiesRefined =[]\n",
    "    notInc = ['density', 'age10yr']\n",
    "    for i in stories:\n",
    "        if i['label'].split(\"_\")[0] not in notInc:\n",
    "            storiesRefined.append(i)\n",
    "            notInc.append(i['label'].split(\"_\")[0])\n",
    "    storiesRefined\n",
    "    \n",
    "    if (thisLad['code'][0]!='W'):\n",
    "        storiesRefined = sorted(storiesRefined, key=lambda x: ((abs(x['locRank'])*(topicScoresOrder[bbcRegLU[thisLad['code']]][x['label'].split('_')[0]])*(abs(x['natRank'])/10)*(5/abs(x['value'])))))\n",
    "\n",
    "    \n",
    "    #Find differences in data\n",
    "    if (thisLad['parents'][0]['name']=='Wales'):                        \n",
    "        region=[i for i in countries if i['name']==\"Wales\"][0]\n",
    "        country=[i for i in countries if i['name']==\"Wales\"][0]\n",
    "    else:\n",
    "        region=[i for i in regions if i['name']==thisLad['parents'][0]['name']][0]\n",
    "        country=[i for i in countries if i['name']==thisLad['parents'][1]['name']][0]\n",
    "\n",
    "    def reg(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return region['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    regDiff = [(i['label'], i['value']-reg(i)) for i in change]\n",
    "    regDiff = sorted(regDiff, reverse=True, key=lambda x: abs(x[1]))\n",
    "\n",
    "    def cou(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return countries[1]['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    couDiff = [(i['label'], i['value']-cou(i)) for i in change]\n",
    "    couDiff = sorted(couDiff, reverse=True, key=lambda x: abs(x[1]))\n",
    "    \n",
    "    try:\n",
    "        nearSimilarData = {}\n",
    "        nearSimilarData['name'] = nearSimilar[0][1]\n",
    "        nearSimilarData['data'] = [lad for lad in lads if lad['name']==nearSimilar[0][1]][0]['data']\n",
    "    except:\n",
    "        nearSimilarData = []\n",
    "\n",
    "    def near(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return nearSimilarData['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    if len(nearSimilarData)>0:\n",
    "        nearDiff = [(i['label'], i['value']-near(i)) for i in change]\n",
    "        nearDiff = sorted(nearDiff, reverse=True, key=lambda x: abs(x[1]))\n",
    "\n",
    "    nearTops = sorted([(lad['name'], lad['pri'][0]) for lad in lads if lad['name'] in [i[1] for i in nearSimilar if i[0]==thisLad['name']]], key=lambda x: abs(x[1]['locRank']))\n",
    "\n",
    "    ageBandChange = sorted([(i, thisLad['data']['age10yr']['value']['2011'][i]-thisLad['data']['age10yr']['value']['2001'][i]) for i in thisLad['data']['age10yr']['value']['2001'].keys()], \n",
    "           reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    equalAgeChange = [lad['name'] for \n",
    "     lad in lads if (lad['parents'][0]['name'] == thisLad['parents'][0]['name']) \n",
    "     & (lad['data']['agemed']['value']['change']['all']==thisLad['data']['agemed']['value']['change']['all'])]\n",
    "    len(equalAgeChange)\n",
    "\n",
    "    # Add data to object\n",
    "    thisLad['stories'] = storiesRefined\n",
    "    thisLad['nearSimilar'] = {}\n",
    "    thisLad['nearSimilar']['triples'] = nearSimilar\n",
    "    thisLad['nearSimilar']['nearTops'] = nearTops\n",
    "    thisLad['nearSimilar']['nearTops'] = nearSimilarData\n",
    "    thisLad['differences'] = {}\n",
    "    thisLad['differences']['country'] = couDiff\n",
    "    thisLad['differences']['region'] = regDiff\n",
    "    thisLad['differences']['near'] = nearDiff\n",
    "    thisLad['data']['age10yr']['absChange'] = ageBandChange\n",
    "    thisLad['data']['agemed']['value_rank_local']['equalAgeChange]'] = len(equalAgeChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "fac0f333-7ebe-4a6e-8274-2927e52d90cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n"
     ]
    }
   ],
   "source": [
    "if (thisLad['parents'][0]['name']=='Wales'):\n",
    "    print('okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "b5f14e20-be68-42c7-a931-158385d79e1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worked:  Hartlepool\n",
      "Worked:  Middlesbrough\n",
      "Worked:  Redcar and Cleveland\n",
      "Worked:  Stockton-on-Tees\n",
      "Worked:  Darlington\n",
      "Worked:  Halton\n",
      "Worked:  Warrington\n",
      "Worked:  Blackburn with Darwen\n",
      "Worked:  Blackpool\n",
      "Worked:  Kingston upon Hull, City of\n",
      "Worked:  East Riding of Yorkshire\n",
      "Worked:  North East Lincolnshire\n",
      "Worked:  North Lincolnshire\n",
      "Worked:  York\n",
      "Worked:  Derby\n",
      "Worked:  Leicester\n",
      "Worked:  Rutland\n",
      "Worked:  Nottingham\n",
      "Worked:  Herefordshire, County of\n",
      "Worked:  Telford and Wrekin\n",
      "Worked:  Stoke-on-Trent\n",
      "Worked:  Bath and North East Somerset\n",
      "Worked:  Bristol, City of\n",
      "Worked:  North Somerset\n",
      "Worked:  South Gloucestershire\n",
      "Worked:  Plymouth\n",
      "Worked:  Torbay\n",
      "Worked:  Swindon\n",
      "Worked:  Peterborough\n",
      "Worked:  Luton\n",
      "Worked:  Southend-on-Sea\n",
      "Worked:  Thurrock\n",
      "Worked:  Medway\n",
      "Worked:  Bracknell Forest\n",
      "Worked:  West Berkshire\n",
      "Worked:  Reading\n",
      "Worked:  Slough\n",
      "Worked:  Windsor and Maidenhead\n",
      "Worked:  Wokingham\n",
      "Worked:  Milton Keynes\n",
      "Worked:  Brighton and Hove\n",
      "Worked:  Portsmouth\n",
      "Worked:  Southampton\n",
      "Worked:  Isle of Wight\n",
      "Worked:  County Durham\n",
      "Worked:  Cheshire East\n",
      "Worked:  Cheshire West and Chester\n",
      "Worked:  Shropshire\n",
      "Worked:  Cornwall\n",
      "Worked:  Isles of Scilly\n",
      "Worked:  Wiltshire\n",
      "Worked:  Bedford\n",
      "Worked:  Central Bedfordshire\n",
      "Worked:  Northumberland\n",
      "Worked:  Bournemouth, Christchurch and Poole\n",
      "Worked:  Dorset\n",
      "Worked:  Buckinghamshire\n",
      "Worked:  Cambridge\n",
      "Worked:  East Cambridgeshire\n",
      "Worked:  Fenland\n",
      "Worked:  Huntingdonshire\n",
      "Worked:  South Cambridgeshire\n",
      "Worked:  Allerdale\n",
      "Worked:  Barrow-in-Furness\n",
      "Worked:  Carlisle\n",
      "Worked:  Copeland\n",
      "Worked:  Eden\n",
      "Worked:  South Lakeland\n",
      "Worked:  Amber Valley\n",
      "Worked:  Bolsover\n",
      "Worked:  Chesterfield\n",
      "Worked:  Derbyshire Dales\n",
      "Worked:  Erewash\n",
      "Worked:  High Peak\n",
      "Worked:  North East Derbyshire\n",
      "Worked:  South Derbyshire\n",
      "Worked:  East Devon\n",
      "Worked:  Exeter\n",
      "Worked:  Mid Devon\n",
      "Worked:  North Devon\n",
      "Worked:  South Hams\n",
      "Worked:  Teignbridge\n",
      "Worked:  Torridge\n",
      "Worked:  West Devon\n",
      "Worked:  Eastbourne\n",
      "Worked:  Hastings\n",
      "Worked:  Lewes\n",
      "Worked:  Rother\n",
      "Worked:  Wealden\n",
      "Worked:  Basildon\n",
      "Worked:  Braintree\n",
      "Worked:  Brentwood\n",
      "Worked:  Castle Point\n",
      "Worked:  Chelmsford\n",
      "Worked:  Colchester\n",
      "Worked:  Epping Forest\n",
      "Worked:  Harlow\n",
      "Worked:  Maldon\n",
      "Worked:  Rochford\n",
      "Worked:  Tendring\n",
      "Worked:  Uttlesford\n",
      "Worked:  Cheltenham\n",
      "Worked:  Cotswold\n",
      "Worked:  Forest of Dean\n",
      "Worked:  Gloucester\n",
      "Worked:  Stroud\n",
      "Worked:  Tewkesbury\n",
      "Worked:  Basingstoke and Deane\n",
      "Worked:  East Hampshire\n",
      "Worked:  Eastleigh\n",
      "Worked:  Fareham\n",
      "Worked:  Gosport\n",
      "Worked:  Hart\n",
      "Worked:  Havant\n",
      "Worked:  New Forest\n",
      "Worked:  Rushmoor\n",
      "Worked:  Test Valley\n",
      "Worked:  Winchester\n",
      "Worked:  Broxbourne\n",
      "Worked:  Dacorum\n",
      "Worked:  Hertsmere\n",
      "Worked:  North Hertfordshire\n",
      "Worked:  Three Rivers\n",
      "Worked:  Watford\n",
      "Worked:  Ashford\n",
      "Worked:  Canterbury\n",
      "Worked:  Dartford\n",
      "Worked:  Dover\n",
      "Worked:  Gravesham\n",
      "Worked:  Maidstone\n",
      "Worked:  Sevenoaks\n",
      "Worked:  Folkestone and Hythe\n",
      "Worked:  Swale\n",
      "Worked:  Thanet\n",
      "Worked:  Tonbridge and Malling\n",
      "Worked:  Tunbridge Wells\n",
      "Worked:  Burnley\n",
      "Worked:  Chorley\n",
      "Worked:  Fylde\n",
      "Worked:  Hyndburn\n",
      "Worked:  Lancaster\n",
      "Worked:  Pendle\n",
      "Worked:  Preston\n",
      "Worked:  Ribble Valley\n",
      "Worked:  Rossendale\n",
      "Worked:  South Ribble\n",
      "Worked:  West Lancashire\n",
      "Worked:  Wyre\n",
      "Worked:  Blaby\n",
      "Worked:  Charnwood\n",
      "Worked:  Harborough\n",
      "Worked:  Hinckley and Bosworth\n",
      "Worked:  Melton\n",
      "Worked:  North West Leicestershire\n",
      "Worked:  Oadby and Wigston\n",
      "Worked:  Boston\n",
      "Worked:  East Lindsey\n",
      "Worked:  Lincoln\n",
      "Worked:  North Kesteven\n",
      "Worked:  South Holland\n",
      "Worked:  South Kesteven\n",
      "Worked:  West Lindsey\n",
      "Worked:  Breckland\n",
      "Worked:  Broadland\n",
      "Worked:  Great Yarmouth\n",
      "Worked:  King's Lynn and West Norfolk\n",
      "Worked:  North Norfolk\n",
      "Worked:  Norwich\n",
      "Worked:  South Norfolk\n",
      "Worked:  Corby\n",
      "Worked:  Daventry\n",
      "Worked:  East Northamptonshire\n",
      "Worked:  Kettering\n",
      "Worked:  Northampton\n",
      "Worked:  South Northamptonshire\n",
      "Worked:  Wellingborough\n",
      "Worked:  Craven\n",
      "Worked:  Hambleton\n",
      "Worked:  Harrogate\n",
      "Worked:  Richmondshire\n",
      "Worked:  Ryedale\n",
      "Worked:  Scarborough\n",
      "Worked:  Selby\n",
      "Worked:  Ashfield\n",
      "Worked:  Bassetlaw\n",
      "Worked:  Broxtowe\n",
      "Worked:  Gedling\n",
      "Worked:  Mansfield\n",
      "Worked:  Newark and Sherwood\n",
      "Worked:  Rushcliffe\n",
      "Worked:  Cherwell\n",
      "Worked:  Oxford\n",
      "Worked:  South Oxfordshire\n",
      "Worked:  Vale of White Horse\n",
      "Worked:  West Oxfordshire\n",
      "Worked:  Mendip\n",
      "Worked:  Sedgemoor\n",
      "Worked:  South Somerset\n",
      "Worked:  Cannock Chase\n",
      "Worked:  East Staffordshire\n",
      "Worked:  Lichfield\n",
      "Worked:  Newcastle-under-Lyme\n",
      "Worked:  South Staffordshire\n",
      "Worked:  Stafford\n",
      "Worked:  Staffordshire Moorlands\n",
      "Worked:  Tamworth\n",
      "Worked:  Babergh\n",
      "Worked:  Ipswich\n",
      "Worked:  Mid Suffolk\n",
      "Worked:  Elmbridge\n",
      "Worked:  Epsom and Ewell\n",
      "Worked:  Guildford\n",
      "Worked:  Mole Valley\n",
      "Worked:  Reigate and Banstead\n",
      "Worked:  Runnymede\n",
      "Worked:  Spelthorne\n",
      "Worked:  Surrey Heath\n",
      "Worked:  Tandridge\n",
      "Worked:  Waverley\n",
      "Worked:  Woking\n",
      "Worked:  North Warwickshire\n",
      "Worked:  Nuneaton and Bedworth\n",
      "Worked:  Rugby\n",
      "Worked:  Stratford-on-Avon\n",
      "Worked:  Warwick\n",
      "Worked:  Adur\n",
      "Worked:  Arun\n",
      "Worked:  Chichester\n",
      "Worked:  Crawley\n",
      "Worked:  Horsham\n",
      "Worked:  Mid Sussex\n",
      "Worked:  Worthing\n",
      "Worked:  Bromsgrove\n",
      "Worked:  Malvern Hills\n",
      "Worked:  Redditch\n",
      "Worked:  Worcester\n",
      "Worked:  Wychavon\n",
      "Worked:  Wyre Forest\n",
      "Worked:  St Albans\n",
      "Worked:  Welwyn Hatfield\n",
      "Worked:  East Hertfordshire\n",
      "Worked:  Stevenage\n",
      "Worked:  East Suffolk\n",
      "Worked:  West Suffolk\n",
      "Worked:  Somerset West and Taunton\n",
      "Worked:  Bolton\n",
      "Worked:  Bury\n",
      "Worked:  Manchester\n",
      "Worked:  Oldham\n",
      "Worked:  Rochdale\n",
      "Worked:  Salford\n",
      "Worked:  Stockport\n",
      "Worked:  Tameside\n",
      "Worked:  Trafford\n",
      "Worked:  Wigan\n",
      "Worked:  Knowsley\n",
      "Worked:  Liverpool\n",
      "Worked:  St. Helens\n",
      "Worked:  Sefton\n",
      "Worked:  Wirral\n",
      "Worked:  Barnsley\n",
      "Worked:  Doncaster\n",
      "Worked:  Rotherham\n",
      "Worked:  Sheffield\n",
      "Worked:  Newcastle upon Tyne\n",
      "Worked:  North Tyneside\n",
      "Worked:  South Tyneside\n",
      "Worked:  Sunderland\n",
      "Worked:  Birmingham\n",
      "Worked:  Coventry\n",
      "Worked:  Dudley\n",
      "Worked:  Sandwell\n",
      "Worked:  Solihull\n",
      "Worked:  Walsall\n",
      "Worked:  Wolverhampton\n",
      "Worked:  Bradford\n",
      "Worked:  Calderdale\n",
      "Worked:  Kirklees\n",
      "Worked:  Leeds\n",
      "Worked:  Wakefield\n",
      "Worked:  Gateshead\n",
      "Worked:  City of London\n",
      "Worked:  Barking and Dagenham\n",
      "Worked:  Barnet\n",
      "Worked:  Bexley\n",
      "Worked:  Brent\n",
      "Worked:  Bromley\n",
      "Worked:  Camden\n",
      "Worked:  Croydon\n",
      "Worked:  Ealing\n",
      "Worked:  Enfield\n",
      "Worked:  Greenwich\n",
      "Worked:  Hackney\n",
      "Worked:  Hammersmith and Fulham\n",
      "Worked:  Haringey\n",
      "Worked:  Harrow\n",
      "Worked:  Havering\n",
      "Worked:  Hillingdon\n",
      "Worked:  Hounslow\n",
      "Worked:  Islington\n",
      "Worked:  Kensington and Chelsea\n",
      "Worked:  Kingston upon Thames\n",
      "Worked:  Lambeth\n",
      "Worked:  Lewisham\n",
      "Worked:  Merton\n",
      "Worked:  Newham\n",
      "Worked:  Redbridge\n",
      "Worked:  Richmond upon Thames\n",
      "Worked:  Southwark\n",
      "Worked:  Sutton\n",
      "Worked:  Tower Hamlets\n",
      "Worked:  Waltham Forest\n",
      "Worked:  Wandsworth\n",
      "Worked:  Westminster\n",
      "Worked:  Isle of Anglesey\n",
      "Worked:  Gwynedd\n",
      "Worked:  Conwy\n",
      "Worked:  Denbighshire\n",
      "Worked:  Flintshire\n",
      "Worked:  Wrexham\n",
      "Worked:  Ceredigion\n",
      "Worked:  Pembrokeshire\n",
      "Worked:  Carmarthenshire\n",
      "Worked:  Swansea\n",
      "Worked:  Neath Port Talbot\n",
      "Worked:  Bridgend\n",
      "Worked:  Vale of Glamorgan\n",
      "Worked:  Cardiff\n",
      "Worked:  Rhondda Cynon Taff\n",
      "Worked:  Caerphilly\n",
      "Worked:  Blaenau Gwent\n",
      "Worked:  Torfaen\n",
      "Worked:  Monmouthshire\n",
      "Worked:  Newport\n",
      "Worked:  Powys\n",
      "Worked:  Merthyr Tydfil\n"
     ]
    }
   ],
   "source": [
    "for lad in lads:\n",
    "    try:\n",
    "        with open('/Users/theojolliffe/Documents/census-data-transformed/json/place/'+lad['code']+'.json', 'w') as outfile:\n",
    "            json.dump(lad, outfile)\n",
    "        print(\"Worked: \", lad['name'])\n",
    "\n",
    "    except:\n",
    "        print(\"Failed: \", lad['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "762931aa-c854-47dd-b956-e9a21721b633",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main c555066] Add files\n",
      " 340 files changed, 41845 insertions(+), 382 deletions(-)\n",
      " create mode 100644 .ipynb_checkpoints/GwyneddPrepro-checkpoint.ipynb\n",
      " create mode 100644 .ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
      " create mode 100644 GwyneddPrepro.ipynb\n",
      " create mode 100644 Untitled.ipynb\n",
      " rewrite json/place/E06000001.json (67%)\n",
      " rewrite json/place/E06000002.json (68%)\n",
      " rewrite json/place/E06000003.json (69%)\n",
      " rewrite json/place/E06000004.json (67%)\n",
      " rewrite json/place/E06000005.json (68%)\n",
      " rewrite json/place/E06000006.json (68%)\n",
      " rewrite json/place/E06000007.json (69%)\n",
      " rewrite json/place/E06000008.json (67%)\n",
      " rewrite json/place/E06000015.json (69%)\n",
      " rewrite json/place/E06000016.json (69%)\n",
      " rewrite json/place/E06000017.json (68%)\n",
      " rewrite json/place/E06000021.json (70%)\n",
      " rewrite json/place/E06000022.json (68%)\n",
      " rewrite json/place/E06000024.json (66%)\n",
      " rewrite json/place/E06000025.json (68%)\n",
      " rewrite json/place/E06000026.json (67%)\n",
      " rewrite json/place/E06000027.json (67%)\n",
      " rewrite json/place/E06000030.json (69%)\n",
      " rewrite json/place/E06000032.json (67%)\n",
      " rewrite json/place/E06000033.json (69%)\n",
      " rewrite json/place/E06000035.json (67%)\n",
      " rewrite json/place/E06000036.json (68%)\n",
      " rewrite json/place/E06000037.json (68%)\n",
      " rewrite json/place/E06000040.json (67%)\n",
      " rewrite json/place/E06000041.json (68%)\n",
      " rewrite json/place/E06000042.json (68%)\n",
      " rewrite json/place/E06000044.json (68%)\n",
      " rewrite json/place/E06000045.json (69%)\n",
      " rewrite json/place/E06000055.json (68%)\n",
      " rewrite json/place/E06000056.json (66%)\n",
      " rewrite json/place/E07000011.json (68%)\n",
      " rewrite json/place/E07000032.json (67%)\n",
      " rewrite json/place/E07000033.json (65%)\n",
      " rewrite json/place/E07000034.json (68%)\n",
      " rewrite json/place/E07000036.json (67%)\n",
      " rewrite json/place/E07000039.json (65%)\n",
      " rewrite json/place/E07000041.json (68%)\n",
      " rewrite json/place/E07000044.json (69%)\n",
      " rewrite json/place/E07000045.json (67%)\n",
      " rewrite json/place/E07000063.json (68%)\n",
      " rewrite json/place/E07000065.json (66%)\n",
      " rewrite json/place/E07000066.json (68%)\n",
      " rewrite json/place/E07000067.json (67%)\n",
      " rewrite json/place/E07000068.json (68%)\n",
      " rewrite json/place/E07000070.json (69%)\n",
      " rewrite json/place/E07000071.json (68%)\n",
      " rewrite json/place/E07000072.json (67%)\n",
      " rewrite json/place/E07000073.json (67%)\n",
      " rewrite json/place/E07000074.json (66%)\n",
      " rewrite json/place/E07000075.json (61%)\n",
      " rewrite json/place/E07000076.json (65%)\n",
      " rewrite json/place/E07000077.json (68%)\n",
      " rewrite json/place/E07000078.json (68%)\n",
      " rewrite json/place/E07000079.json (68%)\n",
      " rewrite json/place/E07000080.json (65%)\n",
      " rewrite json/place/E07000081.json (69%)\n",
      " rewrite json/place/E07000083.json (66%)\n",
      " rewrite json/place/E07000084.json (68%)\n",
      " rewrite json/place/E07000086.json (68%)\n",
      " rewrite json/place/E07000087.json (67%)\n",
      " rewrite json/place/E07000088.json (68%)\n",
      " rewrite json/place/E07000089.json (69%)\n",
      " rewrite json/place/E07000090.json (67%)\n",
      " rewrite json/place/E07000091.json (66%)\n",
      " rewrite json/place/E07000092.json (69%)\n",
      " rewrite json/place/E07000093.json (67%)\n",
      " rewrite json/place/E07000094.json (68%)\n",
      " rewrite json/place/E07000095.json (68%)\n",
      " rewrite json/place/E07000096.json (67%)\n",
      " rewrite json/place/E07000098.json (68%)\n",
      " rewrite json/place/E07000099.json (68%)\n",
      " rewrite json/place/E07000102.json (68%)\n",
      " rewrite json/place/E07000103.json (66%)\n",
      " rewrite json/place/E07000106.json (67%)\n",
      " rewrite json/place/E07000108.json (67%)\n",
      " rewrite json/place/E07000109.json (68%)\n",
      " rewrite json/place/E07000111.json (67%)\n",
      " rewrite json/place/E07000114.json (66%)\n",
      " rewrite json/place/E07000115.json (60%)\n",
      " rewrite json/place/E07000117.json (67%)\n",
      " rewrite json/place/E07000118.json (67%)\n",
      " rewrite json/place/E07000119.json (69%)\n",
      " rewrite json/place/E07000120.json (68%)\n",
      " rewrite json/place/E07000122.json (67%)\n",
      " rewrite json/place/E07000123.json (68%)\n",
      " rewrite json/place/E07000124.json (67%)\n",
      " rewrite json/place/E07000125.json (66%)\n",
      " rewrite json/place/E07000126.json (69%)\n",
      " rewrite json/place/E07000127.json (68%)\n",
      " rewrite json/place/E07000128.json (65%)\n",
      " rewrite json/place/E07000129.json (69%)\n",
      " rewrite json/place/E07000130.json (67%)\n",
      " rewrite json/place/E07000131.json (66%)\n",
      " rewrite json/place/E07000132.json (66%)\n",
      " rewrite json/place/E07000134.json (69%)\n",
      " rewrite json/place/E07000135.json (65%)\n",
      " rewrite json/place/E07000136.json (68%)\n",
      " rewrite json/place/E07000140.json (67%)\n",
      " rewrite json/place/E07000144.json (67%)\n",
      " rewrite json/place/E07000145.json (68%)\n",
      " rewrite json/place/E07000147.json (67%)\n",
      " rewrite json/place/E07000149.json (66%)\n",
      " rewrite json/place/E07000150.json (65%)\n",
      " rewrite json/place/E07000151.json (67%)\n",
      " rewrite json/place/E07000153.json (67%)\n",
      " rewrite json/place/E07000154.json (68%)\n",
      " rewrite json/place/E07000155.json (66%)\n",
      " rewrite json/place/E07000156.json (69%)\n",
      " rewrite json/place/E07000169.json (65%)\n",
      " rewrite json/place/E07000170.json (68%)\n",
      " rewrite json/place/E07000172.json (68%)\n",
      " rewrite json/place/E07000173.json (66%)\n",
      " rewrite json/place/E07000174.json (68%)\n",
      " rewrite json/place/E07000175.json (65%)\n",
      " rewrite json/place/E07000176.json (66%)\n",
      " rewrite json/place/E07000179.json (68%)\n",
      " rewrite json/place/E07000180.json (67%)\n",
      " rewrite json/place/E07000181.json (68%)\n",
      " rewrite json/place/E07000187.json (66%)\n",
      " rewrite json/place/E07000193.json (68%)\n",
      " rewrite json/place/E07000194.json (68%)\n",
      " rewrite json/place/E07000195.json (67%)\n",
      " rewrite json/place/E07000196.json (63%)\n",
      " rewrite json/place/E07000198.json (64%)\n",
      " rewrite json/place/E07000199.json (66%)\n",
      " rewrite json/place/E07000200.json (65%)\n",
      " rewrite json/place/E07000202.json (69%)\n",
      " rewrite json/place/E07000207.json (68%)\n",
      " rewrite json/place/E07000208.json (68%)\n",
      " rewrite json/place/E07000209.json (69%)\n",
      " rewrite json/place/E07000210.json (68%)\n",
      " rewrite json/place/E07000211.json (68%)\n",
      " rewrite json/place/E07000212.json (69%)\n",
      " rewrite json/place/E07000213.json (67%)\n",
      " rewrite json/place/E07000214.json (68%)\n",
      " rewrite json/place/E07000215.json (67%)\n",
      " rewrite json/place/E07000216.json (68%)\n",
      " rewrite json/place/E07000217.json (67%)\n",
      " rewrite json/place/E07000218.json (69%)\n",
      " rewrite json/place/E07000219.json (65%)\n",
      " rewrite json/place/E07000222.json (68%)\n",
      " rewrite json/place/E07000223.json (66%)\n",
      " rewrite json/place/E07000224.json (66%)\n",
      " rewrite json/place/E07000227.json (67%)\n",
      " rewrite json/place/E07000228.json (68%)\n",
      " rewrite json/place/E07000229.json (68%)\n",
      " rewrite json/place/E07000234.json (67%)\n",
      " rewrite json/place/E07000236.json (67%)\n",
      " rewrite json/place/E07000238.json (68%)\n",
      " rewrite json/place/E07000240.json (68%)\n",
      " rewrite json/place/E07000241.json (68%)\n",
      " rewrite json/place/E07000242.json (69%)\n",
      " rewrite json/place/E07000243.json (68%)\n",
      " rewrite json/place/E08000001.json (68%)\n",
      " rewrite json/place/E08000002.json (67%)\n",
      " rewrite json/place/E08000004.json (67%)\n",
      " rewrite json/place/E08000005.json (67%)\n",
      " rewrite json/place/E08000006.json (68%)\n",
      " rewrite json/place/E08000007.json (67%)\n",
      " rewrite json/place/E08000008.json (68%)\n",
      " rewrite json/place/E08000009.json (68%)\n",
      " rewrite json/place/E08000010.json (67%)\n",
      " rewrite json/place/E08000011.json (69%)\n",
      " rewrite json/place/E08000012.json (69%)\n",
      " rewrite json/place/E08000013.json (67%)\n",
      " rewrite json/place/E08000014.json (68%)\n",
      " rewrite json/place/E08000015.json (67%)\n",
      " rewrite json/place/E08000016.json (63%)\n",
      " rewrite json/place/E08000018.json (64%)\n",
      " rewrite json/place/E08000019.json (70%)\n",
      " rewrite json/place/E08000022.json (68%)\n",
      " rewrite json/place/E08000023.json (68%)\n",
      " rewrite json/place/E08000024.json (65%)\n",
      " rewrite json/place/E08000026.json (68%)\n",
      " rewrite json/place/E08000027.json (68%)\n",
      " rewrite json/place/E08000028.json (69%)\n",
      " rewrite json/place/E08000029.json (68%)\n",
      " rewrite json/place/E08000030.json (67%)\n",
      " rewrite json/place/E08000031.json (69%)\n",
      " rewrite json/place/E08000033.json (67%)\n",
      " rewrite json/place/E08000035.json (69%)\n",
      " rewrite json/place/E08000037.json (67%)\n",
      " rewrite json/place/E09000001.json (69%)\n",
      " rewrite json/place/E09000002.json (67%)\n",
      " rewrite json/place/E09000003.json (68%)\n",
      " rewrite json/place/E09000004.json (69%)\n",
      " rewrite json/place/E09000005.json (66%)\n",
      " rewrite json/place/E09000006.json (68%)\n",
      " rewrite json/place/E09000007.json (69%)\n",
      " rewrite json/place/E09000008.json (69%)\n",
      " rewrite json/place/E09000009.json (66%)\n",
      " rewrite json/place/E09000010.json (68%)\n",
      " rewrite json/place/E09000012.json (66%)\n",
      " rewrite json/place/E09000013.json (69%)\n",
      " rewrite json/place/E09000014.json (67%)\n",
      " rewrite json/place/E09000015.json (68%)\n",
      " rewrite json/place/E09000016.json (69%)\n",
      " rewrite json/place/E09000017.json (69%)\n",
      " rewrite json/place/E09000018.json (68%)\n",
      " rewrite json/place/E09000019.json (68%)\n",
      " rewrite json/place/E09000020.json (69%)\n",
      " rewrite json/place/E09000021.json (67%)\n",
      " rewrite json/place/E09000022.json (68%)\n",
      " rewrite json/place/E09000023.json (68%)\n",
      " rewrite json/place/E09000024.json (67%)\n",
      " rewrite json/place/E09000025.json (66%)\n",
      " rewrite json/place/E09000026.json (69%)\n",
      " rewrite json/place/E09000027.json (69%)\n",
      " rewrite json/place/E09000028.json (69%)\n",
      " rewrite json/place/E09000029.json (69%)\n",
      " rewrite json/place/E09000030.json (67%)\n",
      " rewrite json/place/E09000031.json (66%)\n",
      " rewrite json/place/E09000032.json (68%)\n",
      " rewrite json/place/E09000033.json (67%)\n",
      " rewrite json/place/W06000005.json (69%)\n",
      " rewrite json/place/W06000014.json (68%)\n",
      " rewrite json/place/W06000016.json (67%)\n",
      " rewrite json/place/W06000018.json (68%)\n",
      " rewrite json/place/W06000019.json (69%)\n",
      " rewrite json/place/W06000020.json (69%)\n",
      " rewrite json/place/W06000021.json (67%)\n",
      " rewrite json/place/W06000022.json (68%)\n",
      " rewrite json/place/W06000024.json (67%)\n",
      "Enumerating objects: 583, done.\n",
      "Counting objects: 100% (583/583), done.\n",
      "Delta compression using up to 16 threads\n",
      "Compressing objects: 100% (344/344), done.\n",
      "Writing objects: 100% (344/344), 1.01 MiB | 6.06 MiB/s, done.\n",
      "Total 344 (delta 298), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (298/298), completed with 237 local objects.\u001b[K\n",
      "To https://github.com/theojolliffe/census-data.git\n",
      "   e8eec20..c555066  main -> main\n",
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Add files\"\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb35451-fbcb-407c-9978-130600be880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[i for i in lads if i['name']=='Manchester'][0]['stories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "feadbd47-23b7-424b-a00d-2109daebcdcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'travel_perc_change_train_metro',\n",
       "  'locRank': -1,\n",
       "  'natRank': -7,\n",
       "  'value': -4.93},\n",
       " {'label': 'tenure_perc_change_rented_social',\n",
       "  'locRank': -1,\n",
       "  'natRank': -4,\n",
       "  'value': -7.6},\n",
       " {'label': 'health_perc_change_good',\n",
       "  'locRank': 1,\n",
       "  'natRank': 1,\n",
       "  'value': 15.87},\n",
       " {'label': 'economic_perc_change_student',\n",
       "  'locRank': 1,\n",
       "  'natRank': 6,\n",
       "  'value': 2.67},\n",
       " {'label': 'ethnicity_perc_change_black',\n",
       "  'locRank': 1,\n",
       "  'natRank': 8,\n",
       "  'value': 4.13},\n",
       " {'label': 'population_value_change_all',\n",
       "  'locRank': 1,\n",
       "  'natRank': 2,\n",
       "  'value': 28.08},\n",
       " {'label': 'agemed_value_change_all',\n",
       "  'locRank': -1,\n",
       "  'natRank': -3,\n",
       "  'value': -2.0}]"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lads if i['name']=='Manchester'][0]['stories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "de73b04e-40c1-44b6-a4fd-d46669290749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'travel': 0.7621064077360695,\n",
       " 'tenure': 0.7792718330541142,\n",
       " 'health': 0.7902066737468546,\n",
       " 'economic': 0.793664815031433,\n",
       " 'ethnicity': 0.7955981355335854,\n",
       " 'population': 0.8037250405900891,\n",
       " 'agemed': 0.8139850041580589}"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicScoresOrder['manchester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "12bff6b9-92e5-4b03-8602-35846e8a728a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4000000000000001"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*(7/10)*(5/7)*topicRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "d769bf3c-cb5d-4e10-a74c-3b6ef3a92278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(7/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "02e3b46d-4a58-4a0a-b3f0-934a9bb4613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9235a-c0bf-4fde-ad85-a80ebede2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "storiesRefined = sorted(storiesRefined, key=lambda x: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "d5d30fe8-9bc4-46b4-b6d8-0ba81e3f9778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'population_value_change_all',\n",
       "  'locRank': -1,\n",
       "  'natRank': -20,\n",
       "  'value': 0.97},\n",
       " {'label': 'economic_perc_change_self-employed',\n",
       "  'locRank': -1,\n",
       "  'natRank': -30,\n",
       "  'value': 0.63},\n",
       " {'label': 'health_perc_change_bad',\n",
       "  'locRank': -5,\n",
       "  'natRank': -122,\n",
       "  'value': -3.67},\n",
       " {'label': 'agemed_value_change_all',\n",
       "  'locRank': -6,\n",
       "  'natRank': -107,\n",
       "  'value': 0.0}]"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33865d4-3b05-4ef7-b92e-aea3aaa251bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
