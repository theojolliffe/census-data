{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cf777daa-fdff-45c5-bd81-5b10594e464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "from networkx_query import search_nodes, search_edges\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e9186ce5-d80a-4b13-95fb-1497da7a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/theojolliffe/Documents/Wayback BBC/topicScoresOrder.json', 'r') as fp:\n",
    "    topicScoresOrder = json.load(fp)\n",
    "topicScoresOrder = {i: {i[0]: 1/(1+i[1]/10) for i in topicScoresOrder[i]} for i in topicScoresOrder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ec79ce9c-569f-40ad-a509-a2a866a469f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = pd.read_csv('./csv/lists/places_2020.csv')\n",
    "areaType = pd.read_csv('/Users/theojolliffe/Documents/Census Data/censusAreaLookup.csv')\n",
    "typeLookup = {}\n",
    "for i in areaType.index:\n",
    "    typeLookup[areaType.iloc[i]['Name']]=areaType.iloc[i]['Group name']\n",
    "\n",
    "areas = []\n",
    "for i in options[\"code\"]:\n",
    "    try:\n",
    "        areas.append(json.load(open(f'/Users/theojolliffe/Documents/pipeline_v3/final/json/place/{i}.json', 'rb')))\n",
    "    except FileNotFoundError:\n",
    "#         print(i)\n",
    "        pass\n",
    "    \n",
    "for area in areas:\n",
    "    area['data']['agemed']['value']['change']['all'] = area['data']['agemed']['value']['2011']['all']-area['data']['agemed']['value']['2001']['all']\n",
    "    area['data']['density']['value']['change']['all'] = area['data']['density']['value']['2011']['all']-area['data']['density']['value']['2001']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "cfc395f5-e54a-4533-b6a8-9b482632b33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seperate the areas by area type\n",
    "regions = []\n",
    "for i in areas:\n",
    "    if i['type']=='rgn':\n",
    "        regions.append(i)\n",
    "        \n",
    "lads = []\n",
    "for i in areas:\n",
    "    if i['type']=='lad':\n",
    "        lads.append(i)\n",
    "        \n",
    "countries = []\n",
    "for i in areas:\n",
    "    if (i['type']=='ew')|(i['type']=='ctry'):\n",
    "        countries.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a11225ff-a17a-4c22-bae7-bdc029671934",
   "metadata": {},
   "outputs": [],
   "source": [
    "nameChanges = {\n",
    "    \"Herefordshire, County of\": \"Herefordshire\",\n",
    "    \"Bristol, City of\": \"Bristol\",\n",
    "    \"Isle of Anglesey\": \"the Isle of Anglesey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1a8536b7-f31e-4658-ab60-176fabf3de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "simiDF = pd.read_csv('/Users/theojolliffe/Documents/correspondinglocalauthoritiesv3r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "f31f93b1-f9d2-402c-a639-c11d90d706ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfLads = {\"England\": 307, \"Wales\": 22}\n",
    "areaClassDF = pd.read_csv('/Users/theojolliffe/Documents/area-reports/src/data/censusAreaLookup.csv')\n",
    "areaClassDF.at[75, 'Code'] = \"E06000058\"\n",
    "areaClassLU = {\n",
    "    \"1a1r\": \"affluent suburban area\", \"1b1r\": \"affluent rural area\", \n",
    "    \"1b2r\": \"growing rural area\", \"2a1r\": \"city\", \n",
    "    \"2b1r\": \"area\", \"3a2r\": \"remote area\", \n",
    "    \"3a1r\": \"agricultural area\", \"3b1r\": \"coastal area\",\n",
    "    \"3b2r\": \"seaside  area\",\n",
    "    \"4a1r\": \"urban area\", \"5a1r\": \"cosmopolitan area\",\n",
    "    \"6a2r\": \"industrial area\", \"6a3r\": \"service economy area\",\n",
    "    \"6a1r\": \"industrial area\", \"7a1r\": \"rural area\",\n",
    "    \"7c2r\": \"affluent area\", \"7c1r\": \"affluent area\",\n",
    "    \"8a1r\": \"multi-cultural area\", \"8a2r\": \"urban area\",\n",
    "    \"8b1r\": \"suburban area\", \"8b2r\": \"suburban area\",\n",
    "}\n",
    "areaClassSuperLU = {\n",
    "    \"1r\": \"affluent area\",\n",
    "    \"2r\": \"urban area\",\n",
    "    \"3r\": \"rural area\",\n",
    "    \"4r\": \"multi-cultural area\",\n",
    "    \"5r\": \"cosmopolitan area\",\n",
    "    \"6r\": \"industrial area\",\n",
    "    \"7r\": \"suburban area\",\n",
    "    \"8r\": \"built up area\",\n",
    "}\n",
    "gssLookup = {\"E09\": \"London borough\", \"E08\": \"metropolitan district\", \"E07\": \"district\", \"E06\": \"unitary authority\", \"W06\": \"Welsh district\"}\n",
    "gssLookupShort = {\"E09\": \"borough\", \"E08\": \"district\", \"E07\": \"district\", \"E06\": \"unitary authority\", \"W06\": \"district\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f0e2f1d5-7661-4b5b-b627-dbeae732592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbcReg = pd.read_csv('/Users/theojolliffe/Downloads/bbc_regions.csv')\n",
    "bbcRegLU = {}\n",
    "for i in bbcReg.index:\n",
    "    if bbcReg['areaCode'].iloc[i] in [lad['code'] for lad in lads]:\n",
    "        bbcRegLU[bbcReg['areaCode'].iloc[i]] = bbcReg['bbc_online'].iloc[i]\n",
    "bbcRegLU['E06000053'] = 'cornwall'\n",
    "bbcRegLU['E06000060'] = 'beds_bucks_and_herts'\n",
    "bbcRegLU['E09000001'] = 'london'\n",
    "bbcRegLU['E07000150'] = 'northamptonshire'\n",
    "\n",
    "regionsBBC = ['cumbria', 'lancashire', 'merseyside', 'manchester', 'tees', 'tyne_and_wear', 'Humberside', 'york_and_north_yorkshire', 'leeds_and_west_yorkshire', 'lincolnshire', 'south_yorkshire', 'birmingham_and_black_country', 'coventry_and_warwickshire', 'hereford_and_worcester', 'shropshire', 'stoke_and_staffordshire', 'derbyshire', 'leicester', 'northamptonshire', 'nottingham', 'bristol', 'cornwall', 'devon', 'gloucestershire', 'somerset', 'wiltshire', 'beds_bucks_and_herts', 'cambridgeshire', 'essex', 'norfolk', 'suffolk', 'berkshire', 'dorset', 'hampshire', 'oxford', 'kent', 'london', 'surrey', 'sussex']\n",
    "bbcNames = {\n",
    "     'derby': 'derbyshire', \n",
    "     'hereford and worcester': 'hereford_and_worcester',\n",
    "     'tyne and wear': 'tyne_and_wear',\n",
    "     'northampton': 'northamptonshire',\n",
    "     'liverpool': 'merseyside',\n",
    "     'coventry and warwickshire': 'coventry_and_warwickshire',\n",
    "     'humberside': 'Humberside',\n",
    "     'sheffield and south yorkshire': 'south_yorkshire',\n",
    "     'hampshire & isle of wight': 'hampshire',\n",
    "     'stoke and staffordshire': 'stoke_and_staffordshire',\n",
    "     'york & north yorkshire': 'york_and_north_yorkshire',\n",
    "     'birmingham and black country': 'birmingham_and_black_country',\n",
    "     'beds, bucks and herts': 'beds_bucks_and_herts',\n",
    "     'leeds and west yorkshire': 'leeds_and_west_yorkshire'\n",
    "}\n",
    "for i in bbcRegLU.keys():\n",
    "    bbcRegLU[i] = bbcRegLU[i].lower()\n",
    "    if bbcRegLU[i] not in regionsBBC:\n",
    "        try:\n",
    "            bbcRegLU[i] = bbcNames[bbcRegLU[i]]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "8a4fc7f9-c133-40b6-9715-ac4f17389a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('/Users/theojolliffe/Documents/Wayback BBC/BBCRegionalTFIDF.csv')\n",
    "tfidf = tfidf.set_index('Unnamed: 0')\n",
    "for i in tfidf.columns:\n",
    "    for j in tfidf[i].index:\n",
    "        tfidf[i].loc[j] = 1+100*tfidf[i].loc[j]\n",
    "tfidfLU = {}\n",
    "for i in regionsBBC:\n",
    "    tfidfLU[i]=tfidf.loc[i].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f2cbdd62-bf9d-482d-a9c6-67ba1f713c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TFtopicLU = {\n",
    "    'health': 'health',\n",
    "    'racial': 'ethnicity',\n",
    "    'population': 'population',\n",
    "    'age': 'agemed',\n",
    "    'work': 'economic',\n",
    "    'commuter': 'travel',\n",
    "    'housing': 'tenure'\n",
    "}\n",
    "TFtopicRev = dict((TFtopicLU[x],x) for x in TFtopicLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "06b83c92-75bc-4f17-b97a-1500bada38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this(lad):\n",
    "    return[l for l in lads if l['name']==lad][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f218643b-a56a-4227-b87c-28fd07260dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [{1: 'population', 2: 'value', 3: 'all'}, \n",
    "          {1: 'density', 2: 'value', 3: 'all'},\n",
    "          {1: 'agemed', 2: 'value', 3: 'all'},\n",
    "          {1: 'care', 2: 'perc', 3: '1to19hoursWeek'},\n",
    "          {1: 'care', 2: 'perc', 3: '20to49hoursWeek'},\n",
    "          {1: 'care', 2: 'perc', 3: '40PlushoursWeek'},\n",
    "          {1: 'children', 2: 'perc', 3: 'NoKids'},\n",
    "          {1: 'children', 2: 'perc', 3: 'Kids'},\n",
    "          {1: 'children', 2: 'perc', 3: 'NonDepKids'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'car_van'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'bus'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'home'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'train_metro'},\n",
    "          {1: 'travel', 2: 'perc', 3: 'foot'},\n",
    "          {1: 'tenure', 2: 'perc', 3: 'rented_social'},\n",
    "          {1: 'tenure', 2: 'perc', 3: 'rented_private'},\n",
    "          {1: 'tenure', 2: 'perc', 3: 'owned'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Hindu'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Buddhist'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Jewish'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Christian'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Muslim'},\n",
    "          {1: 'religion', 2: 'perc', 3: 'Noreligion'},\n",
    "          {1: 'household', 2: 'perc', 3: 'OnePerson'},\n",
    "          {1: 'household', 2: 'perc', 3: 'Married'},\n",
    "          {1: 'household', 2: 'perc', 3: 'LoneParent'},\n",
    "          {1: 'household', 2: 'perc', 3: 'Cohabiting'},\n",
    "          {1: 'health', 2: 'perc', 3: 'bad'},\n",
    "          {1: 'health', 2: 'perc', 3: 'good'},\n",
    "          {1: 'ethnicity', 2: 'perc', 3: 'black'},\n",
    "          {1: 'ethnicity', 2: 'perc', 3: 'white'},\n",
    "          {1: 'ethnicity', 2: 'perc', 3: 'asian'},\n",
    "          {1: 'marital', 2: 'perc', 3: 'Married'},\n",
    "          {1: 'marital', 2: 'perc', 3: 'Seperated'},\n",
    "          {1: 'marital', 2: 'perc', 3: 'Single'},\n",
    "          {1: 'hoursworked', 2: 'perc', 3: 'Female1-15'},\n",
    "          {1: 'hoursworked', 2: 'perc', 3: 'Male1-15'},\n",
    "          {1: 'hoursworked', 2: 'perc', 3: 'Female49plus'},\n",
    "          {1: 'hoursworked', 2: 'perc', 3: 'Male49plus'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'employee'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'student'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'unemployed'},\n",
    "          {1: 'economic', 2: 'perc', 3: 'self-employed'}]\n",
    "\n",
    "ladCodes = []\n",
    "for lad in lads:\n",
    "    ladCodes.append(lad['code'])\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(columns=\n",
    "                  ['lad', '2001',  '2011',  'change', 'topic', 'parent'])\n",
    "for j in range(len(topics)):\n",
    "    df1 = pd.DataFrame(index=ladCodes, columns=\n",
    "                      ['lad', '2001',  '2011',  'change', 'topic', 'parent'])\n",
    "    for i in range(len(lads)):\n",
    "        df1['lad'].iloc[i] = lads[i]['name']\n",
    "        df1['2001'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2001'][topics[j][3]]\n",
    "        df1['2011'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2011'][topics[j][3]]    \n",
    "        df1['change'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['change'][topics[j][3]]\n",
    "        df1['topic'].iloc[i] = topics[j][1]+\"_\"+topics[j][3]\n",
    "        df1['parent'].iloc[i] = lads[i]['parents'][0]['name']\n",
    "    df = pd.concat([df,df1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c06daf46-f8d7-463e-9baa-448dae6405a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectList = [\"Male31-48\", \"Male16-30\", \"Female31-48\", \"Female16-30\", \"Widowed\", \"65andOver\", \"Other\", \"noCare\", \"Sikh\", \"Religionnotstated\", \"MultiOther\", \"Otherreligion\", \"fair\", \"rent_free\", \"shared_ownership\", \"bicycle\", \"taxi\", \"moto\", \"other\", \"female\", \"male\", \"inactive\", \"mixed\", \"LoneNKids\", \"MultiStudents\"]\n",
    "topicList = [\"age10yr\"]\n",
    "\n",
    "# Find each LAD local and national ranks and put in prioirity order\n",
    "for thisLad in lads:\n",
    "    \n",
    "    try:\n",
    "        thisLad['similar'] = [\n",
    "            {'code': simiDF[simiDF['Code']==thisLad['code']]['Code.1'].iloc[0], 'name': simiDF[simiDF['Code']==thisLad['code']]['Name.1'].iloc[0]},\n",
    "            {'code': simiDF[simiDF['Code']==thisLad['code']]['Code.2'].iloc[0], 'name': simiDF[simiDF['Code']==thisLad['code']]['Name.2'].iloc[0]},\n",
    "            {'code': simiDF[simiDF['Code']==thisLad['code']]['Code.3'].iloc[0], 'name': simiDF[simiDF['Code']==thisLad['code']]['Name.3'].iloc[0]}\n",
    "        ]\n",
    "    except:\n",
    "        thisLad['similar'] = [\n",
    "            {'code': 'faulty pre-process', 'name': 'faulty pre-process'},\n",
    "            {'code': 'faulty pre-process', 'name': 'faulty pre-process'},\n",
    "            {'code': 'faulty pre-process', 'name': 'faulty pre-process'}\n",
    "        ]\n",
    "    \n",
    "    if (areaClassDF[areaClassDF['Code']==thisLad['code']]['Subgroup code'].shape[0]>0):\n",
    "        thisLad['classification'] = areaClassLU[areaClassDF[areaClassDF['Code']==thisLad['code']]['Subgroup code'].iloc[0]]\n",
    "    else:\n",
    "        thisLad['classification'] = \"unk\"\n",
    "    thisLad['gss'] = {}\n",
    "    thisLad['gss']['short'] = gssLookupShort[thisLad['code'][:3]]\n",
    "    thisLad['gss']['long'] = gssLookup[thisLad['code'][:3]]\n",
    "    \n",
    "    if thisLad['parents'][0]['type'] == \"cty\":\n",
    "        thisLad['parents'] = thisLad['parents'][1:]\n",
    "    \n",
    "    # Empty array will be populted with ranks for each variable\n",
    "    ranks = []\n",
    "\n",
    "    # Filter areas with same parent\n",
    "    sister_lads = []\n",
    "    try:\n",
    "        par_name = next(i for i in thisLad['parents'] if ([j for j in i.values()][2] == 'rgn'))['name']\n",
    "    except:\n",
    "        par_name = next(i for i in thatLad['parents'] if ([j for j in i.values()][2] == 'ctry'))['name']\n",
    "        \n",
    "    for thatLad in lads:\n",
    "        try:\n",
    "            if  par_name == next(i for i in thatLad['parents'] if ([j for j in i.values()][2] == 'rgn'))['name']:\n",
    "                sister_lads.append(thatLad)\n",
    "        except:\n",
    "            if par_name=='Wales':\n",
    "                sister_lads.append(thatLad)\n",
    "            \n",
    "    # Loop through the various data variables\n",
    "    for a in thisLad['data']:\n",
    "        if a not in topicList:\n",
    "            if a in ['population', 'density', 'agemed']:\n",
    "                b = 'value'\n",
    "            else:\n",
    "                b = 'perc'\n",
    "            # Create nested object with localised rank\n",
    "            thisLad['data'][a][b+\"_rank_local\"] = {}    \n",
    "            for c in ['2001', '2011', 'change']:\n",
    "                thisLad['data'][a][b+\"_rank_local\"][c] = {}\n",
    "                for d in thisLad['data'][a][b][c]:\n",
    "                    if d not in subjectList:\n",
    "                        vari = thisLad['data'][a][b][c][d]\n",
    "\n",
    "                        # Create sorted list of values from sister areas\n",
    "                        group_values = []\n",
    "                        for lad in sister_lads:\n",
    "                            group_values.append(lad['data'][a][b][c][d])\n",
    "                            group_values = [x if (type(x) == float) | (type(x) == int) else np.nan for x in group_values]\n",
    "                        group_values.sort(reverse=True)\n",
    "\n",
    "                        # Find index of value of area of interest\n",
    "                        varRank = group_values.index(vari) + 1\n",
    "\n",
    "                        # Convert bottom half rankings into negative values\n",
    "                        if varRank>len(group_values)/2:\n",
    "                            varRank = varRank-len(group_values)-1\n",
    "\n",
    "                        if thisLad['parents'][0]['name'] == \"Wales\":\n",
    "                            natRank = [i for i in df[(df['parent']==\"Wales\")&(df['topic']==a+\"_\"+d)].sort_values(by=[c], ascending=False)['lad']].index(thisLad['name'])+1\n",
    "                            if natRank > noOfLads[\"Wales\"]/2:\n",
    "                                natRank = natRank-noOfLads[\"Wales\"]-1\n",
    "                        else:\n",
    "                            natRank = [i for i in df[(df['parent']!=\"Wales\")&(df['topic']==a+\"_\"+d)].sort_values(by=[c], ascending=False)['lad']].index(thisLad['name'])+1\n",
    "                            if natRank > noOfLads[\"England\"]/2:\n",
    "                                natRank = natRank-noOfLads[\"England\"]-1\n",
    "\n",
    "                        thisLad['data'][a][b+\"_rank_local\"][c][d] = varRank\n",
    "                        thisLad['data'][a][b+\"_rank\"][c][d] = natRank\n",
    "\n",
    "                        # Append ranking data to original array\n",
    "                        ranks.append({\n",
    "                            'label': a+'_'+b+'_'+c+'_'+d, \n",
    "                            'locRank': varRank, \n",
    "                            'natRank': natRank, \n",
    "                            'value': vari})\n",
    "\n",
    "    # Sort in rank order\n",
    "    ranks = sorted(ranks, key=lambda x: (abs(x['locRank']), -abs(x['value'])))\n",
    "    thisLad[\"Priorities\"] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "27663250-5802-421e-b0ab-bb731af6224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=\n",
    "                  ['lad', '2001',  '2011',  'change', 'natRank', 'localRank', 'topic', 'parent'])\n",
    "for j in range(len(topics)):\n",
    "    df1 = pd.DataFrame(index=ladCodes, columns=\n",
    "                      ['lad', '2001',  '2011',  'change', 'natRank', 'localRank', 'topic', 'parent'])\n",
    "    for i in range(len(lads)):\n",
    "        df1['lad'].iloc[i] = lads[i]['name']\n",
    "        df1['2001'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2001'][topics[j][3]]\n",
    "        df1['2011'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['2011'][topics[j][3]]    \n",
    "        df1['change'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]]['change'][topics[j][3]]\n",
    "        df1['natRank'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]+\"_rank\"]['change'][topics[j][3]]    \n",
    "        df1['localRank'].iloc[i] = lads[i]['data'][topics[j][1]][topics[j][2]+\"_rank_local\"]['change'][topics[j][3]]    \n",
    "        df1['topic'].iloc[i] = topics[j][1]+\"_\"+topics[j][3]\n",
    "        try:\n",
    "            df1['parent'].iloc[i] = next(k for k in lads[i]['parents'] if ([j for j in k.values()][2] == 'rgn'))['name']\n",
    "        except:\n",
    "            df1['parent'].iloc[i] = next(k for k in lads[i]['parents'] if ([j for j in k.values()][2] == 'ctry'))['name']\n",
    "        \n",
    "    df = pd.concat([df,df1])\n",
    "\n",
    "for topic in topics:\n",
    "    for lad in lads:\n",
    "        \n",
    "        # Find the areas that this area has overtaken\n",
    "        v2001 = lad['data'][topic[1]][topic[2]]['2001'][topic[3]]\n",
    "        v2011 = lad['data'][topic[1]][topic[2]]['2011'][topic[3]]\n",
    "        if thisLad['parents'][0]['name'] == \"Wales\":\n",
    "            dfT = df[(df['parent']==\"Wales\")&(df['topic']==topic[1]+\"_\"+topic[3])&(df['2001']>v2001)&(df['2011']<v2011)]\n",
    "        else:\n",
    "            dfT = df[(df['parent']!=\"Wales\")&(df['topic']==topic[1]+\"_\"+topic[3])&(df['2001']>v2001)&(df['2011']<v2011)]\n",
    "\n",
    "        obje = []\n",
    "        for i in range(dfT.shape[0]):\n",
    "            obje.append(dfT.iloc[i]['lad'])\n",
    "        \n",
    "        if 'overtake' not in lad['data'][topic[1]][topic[2]+'_rank'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank']['overtake'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+\"_rank\"]['overtake'][topic[3]] = obje\n",
    "        \n",
    "        \n",
    "        # Find areas within the region that this area has overtaken\n",
    "        reg = lad['parents'][0]['name']\n",
    "        dfTr = df[(df['topic']==topic[1]+\"_\"+topic[3])&(df['2001']>v2001)&(df['2011']<v2011)&(df['parent']==reg)]\n",
    "        objeR = []\n",
    "        for i in range(dfTr.shape[0]):\n",
    "            objeR.append(dfTr.iloc[i]['lad'])\n",
    "        if 'overtake' not in lad['data'][topic[1]][topic[2]+'_rank_local'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['overtake'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+\"_rank_local\"]['overtake'][topic[3]] = objeR\n",
    "        \n",
    "        dfReg = df[(df['topic']==topic[1]+\"_\"+topic[3])&(df['parent']==reg)]\n",
    "        rank2001 = sorted(dfReg['2001'], reverse=True).index(v2001)+1\n",
    "        if rank2001 > 168:\n",
    "            rank2001 = rank2001-336-1\n",
    "        if '2001' not in lad['data'][topic[1]][topic[2]+'_rank_local'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['2001'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['2001'][topic[3]] = rank2001\n",
    "        \n",
    "        \n",
    "        # Find the area immediatly above or below\n",
    "        above = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]-2\n",
    "        below = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]\n",
    "        if below<0:\n",
    "            above = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]-1\n",
    "            below = lad['data'][topic[1]][topic[2]+'_rank_local']['2011'][topic[3]]+1\n",
    "            \n",
    "        name_above = df[(df['parent']==reg)&(df['topic']==topic[1]+\"_\"+topic[3])].sort_values('2011', ascending=False).iloc[above]['lad']\n",
    "        name_below = df[(df['parent']==reg)&(df['topic']==topic[1]+\"_\"+topic[3])].sort_values('2011', ascending=False).iloc[below]['lad']\n",
    "\n",
    "        area_above = {'name': name_above,\n",
    "                         'value': df[(df['lad']==name_above)&(df['topic']==topic[1]+\"_\"+topic[3])]['2011'].iloc[0]}\n",
    "        area_below = {'name': name_below,\n",
    "                          'value': df[(df['lad']==name_below)&(df['topic']==topic[1]+\"_\"+topic[3])]['2011'].iloc[0]}\n",
    "\n",
    "        \n",
    "        if 'above_below' not in lad['data'][topic[1]][topic[2]+'_rank_local'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'] = {}\n",
    "\n",
    "        if topic[3] not in lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]] = {}\n",
    "        \n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]][\"above\"] = area_above\n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['above_below'][topic[3]][\"below\"] = area_below\n",
    "            \n",
    "        # Add top and bottom three biggest movers for each subject to every area\n",
    "        if thisLad['parents'][0]['name'] == \"Wales\":\n",
    "            df_topic = df[(df['parent']==\"Wales\")&(df['topic']==topic[1]+\"_\"+topic[3])&(abs(df['natRank'])<4)]\n",
    "        else:\n",
    "            df_topic = df[(df['parent']!=\"Wales\")&(df['topic']==topic[1]+\"_\"+topic[3])&(abs(df['natRank'])<4)]\n",
    "\n",
    "        ob = {}\n",
    "        for index, row in df_topic.iterrows():\n",
    "            ob[int(row['natRank'])] = {'name': row['lad'], 2001: row['2001'], 2011: row['2011'], 'change': row['change']}\n",
    "        if 'top_bottom' not in lad['data'][topic[1]][topic[2]+'_rank'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank']['top_bottom'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+'_rank']['top_bottom'][topic[3]]=ob\n",
    "        \n",
    "        \n",
    "        # Add top and bottom three biggest movers for each subject to every area (regional)\n",
    "        df_topicReg = df[(df['topic']==topic[1]+\"_\"+topic[3])&(abs(df['localRank'])<4)&(df['parent']==reg)]\n",
    "\n",
    "        ob = {}\n",
    "        for index, row in df_topicReg.iterrows():\n",
    "            ob[int(row['localRank'])] = {'name': row['lad'], 2001: row['2001'], 2011: row['2011'], 'change': row['change']}\n",
    "        if 'top_bottom' not in lad['data'][topic[1]][topic[2]+'_rank_local'].keys():\n",
    "            lad['data'][topic[1]][topic[2]+'_rank_local']['top_bottom'] = {}\n",
    "        lad['data'][topic[1]][topic[2]+'_rank_local']['top_bottom'][topic[3]]=ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "869dc8a0-b2da-4272-b2fa-3c047bb7844f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lad</th>\n",
       "      <th>2001</th>\n",
       "      <th>2011</th>\n",
       "      <th>change</th>\n",
       "      <th>natRank</th>\n",
       "      <th>localRank</th>\n",
       "      <th>topic</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E06000001</th>\n",
       "      <td>Hartlepool</td>\n",
       "      <td>88611.0</td>\n",
       "      <td>92028.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-73</td>\n",
       "      <td>-6</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000002</th>\n",
       "      <td>Middlesbrough</td>\n",
       "      <td>134855.0</td>\n",
       "      <td>138412.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-43</td>\n",
       "      <td>-4</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000003</th>\n",
       "      <td>Redcar and Cleveland</td>\n",
       "      <td>139132.0</td>\n",
       "      <td>135177.0</td>\n",
       "      <td>-2.84</td>\n",
       "      <td>-5</td>\n",
       "      <td>-2</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000004</th>\n",
       "      <td>Stockton-on-Tees</td>\n",
       "      <td>178408.0</td>\n",
       "      <td>191610.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000005</th>\n",
       "      <td>Darlington</td>\n",
       "      <td>97838.0</td>\n",
       "      <td>105564.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000020</th>\n",
       "      <td>Torfaen</td>\n",
       "      <td>4.93</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000021</th>\n",
       "      <td>Monmouthshire</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.04</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000022</th>\n",
       "      <td>Newport</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1.06</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000023</th>\n",
       "      <td>Powys</td>\n",
       "      <td>16.81</td>\n",
       "      <td>17.39</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000024</th>\n",
       "      <td>Merthyr Tydfil</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.74</td>\n",
       "      <td>1.47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14147 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            lad      2001      2011 change natRank localRank  \\\n",
       "E06000001            Hartlepool   88611.0   92028.0   3.86     -73        -6   \n",
       "E06000002         Middlesbrough  134855.0  138412.0   2.64     -43        -4   \n",
       "E06000003  Redcar and Cleveland  139132.0  135177.0  -2.84      -5        -2   \n",
       "E06000004      Stockton-on-Tees  178408.0  191610.0    7.4     128         3   \n",
       "E06000005            Darlington   97838.0  105564.0    7.9     118         2   \n",
       "...                         ...       ...       ...    ...     ...       ...   \n",
       "W06000020               Torfaen      4.93      5.83    0.9     -10       -10   \n",
       "W06000021         Monmouthshire     10.49     12.04   1.55       1         1   \n",
       "W06000022               Newport       5.5      6.56   1.06       9         9   \n",
       "W06000023                 Powys     16.81     17.39   0.58      -4        -4   \n",
       "W06000024        Merthyr Tydfil      4.27      5.74   1.47       2         2   \n",
       "\n",
       "                            topic      parent  \n",
       "E06000001          population_all  North East  \n",
       "E06000002          population_all  North East  \n",
       "E06000003          population_all  North East  \n",
       "E06000004          population_all  North East  \n",
       "E06000005          population_all  North East  \n",
       "...                           ...         ...  \n",
       "W06000020  economic_self-employed       Wales  \n",
       "W06000021  economic_self-employed       Wales  \n",
       "W06000022  economic_self-employed       Wales  \n",
       "W06000023  economic_self-employed       Wales  \n",
       "W06000024  economic_self-employed       Wales  \n",
       "\n",
       "[14147 rows x 8 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5da7a6ad-ffc2-4884-b872-2105329034f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lads_copy = lads.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad131dc-53d5-4c94-892b-0d1fd48ddf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "aa49dda0-4a69-464c-9aa5-c46f7b3ce661",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thisLad in lads:\n",
    "    thisLad['similarC'] = thisLad['similar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "71d1593c-e42a-40a7-97f8-0e64a03bb693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar data not found for::  Cornwall\n",
      "Nearby data not found for::  Isles of Scilly\n",
      "Similar data not found for::  Isles of Scilly\n",
      "Similar data not found for::  Bournemouth, Christchurch and Poole\n",
      "Similar data not found for::  Dorset\n",
      "Similar data not found for::  Buckinghamshire\n",
      "Similar data not found for::  East Suffolk\n",
      "Similar data not found for::  West Suffolk\n",
      "Similar data not found for::  Somerset West and Taunton\n"
     ]
    }
   ],
   "source": [
    "lads = lads_copy.copy()\n",
    "# Filter out priority list by subject\n",
    "for lad in lads:\n",
    "    subjectList = [\"Male31-48\", \"Male16-30\", \"Female31-48\", \"Female16-30\", \"Widowed\", \"65andOver\", \"Other\", \"noCare\", \"Sikh\", \"Religionnotstated\", \"MultiOther\", \"Otherreligion\", \"fair\", \"rent_free\", \"shared_ownership\", \"bicycle\", \"taxi\", \"moto\", \"other\", \"female\", \"male\", \"inactive\", \"mixed\", \"LoneNKids\", \"MultiStudents\"]\n",
    "    priorities = []\n",
    "    priorities2011 = []\n",
    "    for rank in lad['Priorities']:\n",
    "        s=rank['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        if ((s[2] == \"change\") & (s[3] not in subjectList) & (((s[0]==\"household\")&(s[3]==\"Married\")) is False)):\n",
    "            rank['2011']=[i for i in this(\"Manchester\")['Priorities'] if i['label']==s[0]+'_'+s[1]+'_2011_'+s[3]][0]['value']\n",
    "            priorities.append(rank)\n",
    "        if (s[2]!=\"change\"):\n",
    "            priorities2011.append(rank)\n",
    "    lad['pri'] = priorities\n",
    "    lad['pri2011'] = priorities2011\n",
    "\n",
    "\n",
    "# Create triple data for areas of closest proximity\n",
    "geogTriples = []\n",
    "for lad in lads:\n",
    "    list1 = lad['bounds'][0]+lad['bounds'][1]\n",
    "    for lad2 in lads:\n",
    "        list2 = lad2['bounds'][0]+lad2['bounds'][1]\n",
    "        listDif = [abs(list1[i]-list2[i]) for i in [0,1,2,3]]\n",
    "        listDif.sort()\n",
    "        if (sum(listDif[:3]) < 1) & (lad!=lad2):\n",
    "            geogTriples.append([lad['name'], lad2['name'], (\"near\", round(sum(listDif[:3]), 2))])\n",
    "\n",
    "\n",
    "sign = lambda x: math.copysign(1, x)\n",
    "\n",
    "for thisLad in lads:\n",
    "    thisLad['similar'] = thisLad['similarC']\n",
    "    \n",
    "    # Find nearby area of same area type\n",
    "    nearSimilar = []\n",
    "    for i in geogTriples:\n",
    "        try:\n",
    "            if ((i[0]==thisLad['name'])&(typeLookup[i[1]]==typeLookup[thisLad['name']])):\n",
    "                nearSimilar.append([i[0], i[1], ('near_similar', i[2][1])])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(nearSimilar)==0:\n",
    "        nearSimilar = [i for i in geogTriples if i[0]==thisLad['name']]\n",
    "\n",
    "    nearSimilar = sorted(nearSimilar, key=lambda x: x[2][1])\n",
    "\n",
    "    \n",
    "    #Find differences in data\n",
    "    if (thisLad['parents'][0]['name']=='Wales'):                     \n",
    "        region=[i for i in countries if i['name']==\"Wales\"][0]\n",
    "        country=[i for i in countries if i['name']==\"Wales\"][0]\n",
    "    else:\n",
    "        region=[i for i in regions if i['name']==thisLad['parents'][0]['name']][0]\n",
    "        country=[i for i in countries if i['name']==thisLad['parents'][1]['name']][0]\n",
    "\n",
    "    def reg(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return region['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    regDiff = [(i, {'val': reg(i), 'dif': i['value']-reg(i)}) for i in thisLad['pri']]\n",
    "    regDiff = sorted(regDiff, reverse=True, key=lambda x: abs(x[1]['dif']))\n",
    "\n",
    "    def cou(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return country['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    couDiff = [(i, {'val': cou(i), 'dif': i['value']-cou(i)}) for i in thisLad['pri']]\n",
    "    couDiff = sorted(couDiff, reverse=True, key=lambda x: abs(x[1]['dif']))\n",
    "    \n",
    "    try:\n",
    "        nearSimilarData = {}\n",
    "        nearSimilarData['name'] = nearSimilar[0][1]\n",
    "        nearSimilarData['data'] = [lad for lad in lads if lad['name']==nearSimilar[0][1]][0]['data']\n",
    "    except:\n",
    "        print(\"Nearby data not found for:: \", thisLad['name'])\n",
    "        nearSimilarData = []\n",
    "        \n",
    "    try:\n",
    "        similarData = {}\n",
    "        if (nearSimilar[0][1]!=thisLad['similar'][0]['name']):\n",
    "            try:\n",
    "                similarData['name'] = thisLad['similar'][0]['name']\n",
    "                similarData['data'] = [lad for lad in lads if lad['name']==thisLad['similar'][0]['name']][0]['data']\n",
    "            except:\n",
    "                try:\n",
    "                    similarData['name'] = thisLad['similar'][1]['name']\n",
    "                    similarData['data'] = [lad for lad in lads if lad['name']==thisLad['similar'][1]['name']][0]['data']\n",
    "                except:\n",
    "                    similarData['name'] = thisLad['similar'][2]['name']\n",
    "                    similarData['data'] = [lad for lad in lads if lad['name']==thisLad['similar'][2]['name']][0]['data']\n",
    "        else:\n",
    "            try:\n",
    "                similarData['name'] = thisLad['similar'][1]['name']\n",
    "                similarData['data'] = [lad for lad in lads if lad['name']==thisLad['similar'][1]['name']][0]['data']\n",
    "            except:\n",
    "                similarData['name'] = thisLad['similar'][2]['name']\n",
    "                similarData['data'] = [lad for lad in lads if lad['name']==thisLad['similar'][2]['name']][0]['data']\n",
    "    \n",
    "    except:\n",
    "        print(\"Similar data not found for:: \", thisLad['name'])\n",
    "        similarData = []\n",
    "    \n",
    "    def simi(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return similarData['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    if len(similarData)>0:\n",
    "        simiDiff = [(i, {'val': simi(i), 'dif': i['value']-simi(i)}) for i in thisLad['pri']]\n",
    "        simiDiff = sorted(simiDiff, reverse=True, key=lambda x: abs(x[1]['dif']))\n",
    "\n",
    "\n",
    "    def near(i):\n",
    "        s=i['label'].split(\"_\")\n",
    "        if len(s)>4:\n",
    "            s[3] = s[3] + \"_\" + s[4]\n",
    "        return nearSimilarData['data'][s[0]][s[1]][s[2]][s[3]]\n",
    "    if len(nearSimilarData)>0:\n",
    "        nearDiff = [(i, {'val': near(i), 'dif': i['value']-near(i)}) for i in thisLad['pri']]\n",
    "        nearDiff = sorted(nearDiff, reverse=True, key=lambda x: abs(x[1]['dif']))\n",
    "\n",
    "    nearTops = sorted([(lad['name'], lad['pri'][0]) for lad in lads if lad['name'] in [i[1] for i in nearSimilar if i[0]==thisLad['name']]], key=lambda x: abs(x[1]['locRank']))\n",
    "\n",
    "    ageBandChange = sorted([(i, thisLad['data']['age10yr']['value']['2011'][i]-thisLad['data']['age10yr']['value']['2001'][i]) for i in thisLad['data']['age10yr']['value']['2001'].keys()], \n",
    "           reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    equalAgeChange = [lad['name'] for \n",
    "     lad in lads if (lad['parents'][0]['name'] == thisLad['parents'][0]['name']) \n",
    "     & (lad['data']['agemed']['value']['change']['all']==thisLad['data']['agemed']['value']['change']['all'])]\n",
    "    len(equalAgeChange)\n",
    "    \n",
    "    \n",
    "    # Find and refine stories\n",
    "    stories = []\n",
    "    stories=stories+[{**i, **{'type':['pop']}} for i in thisLad['pri'] if i['label'] == 'population_value_change_all']\n",
    "    stories=stories+[{**i, **{'type':['size']}} for i in thisLad['pri'] if ((abs(i['value'])/abs(i['2011'])>0.1)&(abs(i['value']) > 1))]\n",
    "    stories=stories+[{**i, **{'type':['locRank']}} for i in thisLad['pri'] if (abs(i['locRank']) < 4)&(abs(i['value']) > 1)]\n",
    "    stories=stories+[{**i, **{'type':['natRank']}} for i in thisLad['pri'] if (abs(i['natRank']) < 6)&(abs(i['value']) > 1)]\n",
    "    stories=stories+[{**i[0], **{'type':['couBuck']}} for i in couDiff if ((abs(i[1]['val'])>0.5) & (abs(i[0]['value'])>0.5) & (sign(i[1]['val'])!=sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['regBuck']}} for i in regDiff if ((abs(i[1]['val'])>0.5) & (abs(i[0]['value'])>0.5) & (sign(i[1]['val'])!=sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['nearBuck']}} for i in nearDiff if ((abs(i[1]['val'])>0.8) & (abs(i[0]['value'])>0.8) & (sign(i[1]['val'])!=sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['simiBuck']}} for i in simiDiff if ((abs(i[1]['val'])>0.8) & (abs(i[0]['value'])>0.8) & (sign(i[1]['val'])!=sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['couDiff']}} for i in couDiff if ((abs(i[1]['dif'])>2) & (sign(i[1]['val'])==sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['regDiff']}} for i in regDiff if ((abs(i[1]['dif'])>2) & (sign(i[1]['val'])==sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['nearDiff']}} for i in nearDiff if ((abs(i[1]['dif'])>2) & (sign(i[1]['val'])==sign(i[0]['value'])))]\n",
    "    stories=stories+[{**i[0], **{'type':['simiDiff']}} for i in simiDiff if ((abs(i[1]['dif'])>2) & (sign(i[1]['val'])==sign(i[0]['value'])))]\n",
    "    seen = set()\n",
    "    uniq = [item['label'] for item in stories if item['label'] in seen or seen.add(item['label'])]\n",
    "    listofdoubles = [[i for i in stories if i['label'] == uniqI] for uniqI in uniq]\n",
    "    multi = [{**double[0], **{'type': [item for sublist in [i['type'] for i in double] for item in sublist]}} for double in listofdoubles]\n",
    "    stories = [item for item in stories if item['label'] not in [i['label'] for i in multi]]\n",
    "    stories=stories+multi\n",
    "    \n",
    "    storiesRefined =[]\n",
    "    notInc = ['density', 'age10yr', 'travel']\n",
    "    for i in stories:\n",
    "        if i['label'].split(\"_\")[0] not in notInc:\n",
    "            storiesRefined.append(i)\n",
    "            notInc.append(i['label'].split(\"_\")[0])\n",
    "\n",
    "    if (thisLad['code'][0]!='W'):\n",
    "        storiesRefined = sorted(storiesRefined, key=lambda x: ((topicScoresOrder[bbcRegLU[thisLad['code']]][x['label'].split('_')[0]])*(1/(1+abs(x['value'])))))\n",
    "    else:\n",
    "        storiesRefined = sorted(storiesRefined, key=lambda x: 1/(1+abs(x['value'])))\n",
    "\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['locRank']<5)\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['natRank']<5)\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['type']=='regBuck')\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['type']=='couBuck')\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['label']=='population_value_change_all')\n",
    "    storiesRefined = storiesRefined[:6]\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['label']=='agemed_value_change_all')\n",
    "    storiesRefined = sorted(storiesRefined, reverse=True, key=lambda x: x['label']=='population_value_change_all')\n",
    "\n",
    "    \n",
    "    # Create a varibale 'hiRank' which holds the index of a top 5 rank at the national or reginal level to be used for an overtake story\n",
    "    hiRank ={}\n",
    "    hiRankNat = []\n",
    "    for i in storiesRefined:\n",
    "        if i['label'] not in ['population_value_change_all', 'agemed_value_change_all']:\n",
    "            s=i['label'].split(\"_\")\n",
    "            if len(s)>4:\n",
    "                s[3] = s[3] + \"_\" + s[4]\n",
    "            if thisLad['data'][s[0]][s[1]+\"_rank\"]['2011'][s[3]]!=thisLad['data'][s[0]][s[1]+\"_rank\"]['2001'][s[3]]:\n",
    "                hiRankNat.append(abs(thisLad['data'][s[0]][s[1]+\"_rank\"]['2011'][s[3]]))\n",
    "            else:\n",
    "                hiRankNat.append(100)\n",
    "        else:\n",
    "            hiRankNat.append(100)\n",
    "\n",
    "    hiRankReg = []\n",
    "    for i in storiesRefined:\n",
    "        if i['label'] not in ['population_value_change_all', 'agemed_value_change_all']:\n",
    "            s=i['label'].split(\"_\")\n",
    "            if len(s)>4:\n",
    "                s[3] = s[3] + \"_\" + s[4]\n",
    "            if thisLad['data'][s[0]][s[1]+\"_rank_local\"]['2011'][s[3]]!=thisLad['data'][s[0]][s[1]+\"_rank_local\"]['2001'][s[3]]:\n",
    "                hiRankReg.append(abs(thisLad['data'][s[0]][s[1]+\"_rank_local\"]['2011'][s[3]]))\n",
    "            else:\n",
    "                hiRankReg.append(100)\n",
    "        else:\n",
    "            hiRankReg.append(100)\n",
    "            \n",
    "    if min(hiRankNat)<5:\n",
    "        hiRank['rankIn'] = hiRankNat.index(min(hiRankNat))\n",
    "        hiRank['type'] = 'nat'\n",
    "    elif min(hiRankReg)<5:\n",
    "        hiRank['rankIn'] = hiRankReg.index(min(hiRankReg))\n",
    "        hiRank['type'] = 'reg'\n",
    "    elif min(hiRankNat)<10:\n",
    "        hiRank['rankIn'] = hiRankNat.index(min(hiRankNat))\n",
    "        hiRank['type'] = 'nat'\n",
    "    else:\n",
    "        hiRank['rankIn'] = \"None\"\n",
    "        hiRank['type'] = \"None\"\n",
    "        \n",
    "    ageBandChange= [i for i in ageBandChange if i[0]!='all']\n",
    "    ageBandPos = sorted([i for i in ageBandChange if i[1]>0], reverse=True, key=lambda x: abs(x[1]))\n",
    "    ageBandNeg = sorted([i for i in ageBandChange if i[1]<0], reverse=True, key=lambda x: abs(x[1]))\n",
    "    \n",
    "\n",
    "    # Add data to object\n",
    "    thisLad['hiRank']=hiRank\n",
    "    thisLad['stories'] = storiesRefined\n",
    "    thisLad['similar'] = similarData\n",
    "    thisLad['nearSimilar'] = {}\n",
    "    thisLad['nearSimilar']['triples'] = nearSimilar\n",
    "    thisLad['nearSimilar']['nearTops'] = nearTops\n",
    "    thisLad['nearSimilar']['nearTops'] = nearSimilarData\n",
    "    thisLad['differences'] = {}\n",
    "    thisLad['differences']['country'] = couDiff\n",
    "    thisLad['differences']['region'] = regDiff\n",
    "    thisLad['differences']['near'] = nearDiff\n",
    "    thisLad['data']['age10yr']['absChange'] = {}\n",
    "    thisLad['data']['age10yr']['absChange']['pos'] = ageBandPos\n",
    "    thisLad['data']['age10yr']['absChange']['neg'] = ageBandNeg\n",
    "    thisLad['data']['agemed']['value_rank_local']['equalAgeChange]'] = len(equalAgeChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "386c61dd-d3ad-439e-80bc-e61e5dfb755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this(\"Amber Valley\")['data']['density']['value_rank_local']['2011']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c2e0d1b1-f744-4f20-a2db-6efa4d349689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.61"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this(\"Amber Valley\")['data']['density']['value']['2011']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "afac57c5-3760-414e-8006-6bf3e28a1094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-171"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([l for l in lads if (l['data']['density']['value']['2011']['all']<4.61)])-307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac844222-8342-4951-8a8d-56980ef45147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "16d64167-4b6b-42e8-b6d2-96bec351675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rgn in regions:\n",
    "    try:\n",
    "        with open('/Users/theojolliffe/Documents/census-data-transformed/json/place/'+rgn['code']+'.json', 'w') as outfile:\n",
    "            json.dump(rgn, outfile)\n",
    "    except:\n",
    "        print(\"Failed: \", rgn['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "485fa6c6-ffe0-4149-9d87-c588e7c172a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctry in countries:\n",
    "    try:\n",
    "        with open('/Users/theojolliffe/Documents/census-data-transformed/json/place/'+ctry['code']+'.json', 'w') as outfile:\n",
    "            json.dump(ctry, outfile)\n",
    "    except:\n",
    "        print(\"Failed: \", ctry['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b5f14e20-be68-42c7-a931-158385d79e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lad in lads:\n",
    "    try:\n",
    "        with open('/Users/theojolliffe/Documents/census-data-transformed/json/place/'+lad['code']+'.json', 'w') as outfile:\n",
    "            json.dump(lad, outfile)\n",
    "    except:\n",
    "        print(\"Failed: \", lad['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d6f83ae8-3866-49e4-ac5d-320ca9b59a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('laddata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "762931aa-c854-47dd-b956-e9a21721b633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main e0defab4] Add files\n",
      " 331 files changed, 14991 insertions(+), 488 deletions(-)\n",
      " rewrite json/place/E06000001.json (95%)\n",
      " rewrite json/place/E06000002.json (95%)\n",
      " rewrite json/place/E06000003.json (95%)\n",
      " rewrite json/place/E06000004.json (92%)\n",
      " rewrite json/place/E06000005.json (86%)\n",
      " rewrite json/place/E06000006.json (95%)\n",
      " rewrite json/place/E06000007.json (95%)\n",
      " rewrite json/place/E06000008.json (93%)\n",
      " rewrite json/place/E06000009.json (95%)\n",
      " rewrite json/place/E06000010.json (95%)\n",
      " rewrite json/place/E06000011.json (94%)\n",
      " rewrite json/place/E06000012.json (94%)\n",
      " rewrite json/place/E06000013.json (94%)\n",
      " rewrite json/place/E06000014.json (95%)\n",
      " rewrite json/place/E06000015.json (94%)\n",
      " rewrite json/place/E06000016.json (94%)\n",
      " rewrite json/place/E06000017.json (94%)\n",
      " rewrite json/place/E06000018.json (93%)\n",
      " rewrite json/place/E06000019.json (94%)\n",
      " rewrite json/place/E06000020.json (95%)\n",
      " rewrite json/place/E06000021.json (95%)\n",
      " rewrite json/place/E06000022.json (94%)\n",
      " rewrite json/place/E06000023.json (94%)\n",
      " rewrite json/place/E06000024.json (95%)\n",
      " rewrite json/place/E06000025.json (94%)\n",
      " rewrite json/place/E06000026.json (93%)\n",
      " rewrite json/place/E06000027.json (95%)\n",
      " rewrite json/place/E06000030.json (95%)\n",
      " rewrite json/place/E06000031.json (94%)\n",
      " rewrite json/place/E06000032.json (95%)\n",
      " rewrite json/place/E06000033.json (95%)\n",
      " rewrite json/place/E06000034.json (93%)\n",
      " rewrite json/place/E06000035.json (95%)\n",
      " rewrite json/place/E06000036.json (95%)\n",
      " rewrite json/place/E06000037.json (94%)\n",
      " rewrite json/place/E06000038.json (95%)\n",
      " rewrite json/place/E06000039.json (94%)\n",
      " rewrite json/place/E06000040.json (95%)\n",
      " rewrite json/place/E06000041.json (95%)\n",
      " rewrite json/place/E06000042.json (94%)\n",
      " rewrite json/place/E06000043.json (93%)\n",
      " rewrite json/place/E06000044.json (95%)\n",
      " rewrite json/place/E06000045.json (94%)\n",
      " rewrite json/place/E06000046.json (95%)\n",
      " rewrite json/place/E06000047.json (83%)\n",
      " rewrite json/place/E06000049.json (94%)\n",
      " rewrite json/place/E06000050.json (94%)\n",
      " rewrite json/place/E06000051.json (94%)\n",
      " rewrite json/place/E06000052.json (93%)\n",
      " rewrite json/place/E06000053.json (96%)\n",
      " rewrite json/place/E06000054.json (94%)\n",
      " rewrite json/place/E06000055.json (95%)\n",
      " rewrite json/place/E06000056.json (94%)\n",
      " rewrite json/place/E06000057.json (94%)\n",
      " rewrite json/place/E06000058.json (95%)\n",
      " rewrite json/place/E06000059.json (95%)\n",
      " rewrite json/place/E06000060.json (94%)\n",
      " rewrite json/place/E07000008.json (95%)\n",
      " rewrite json/place/E07000009.json (95%)\n",
      " rewrite json/place/E07000010.json (95%)\n",
      " rewrite json/place/E07000011.json (95%)\n",
      " rewrite json/place/E07000012.json (94%)\n",
      " rewrite json/place/E07000026.json (94%)\n",
      " rewrite json/place/E07000027.json (95%)\n",
      " rewrite json/place/E07000028.json (94%)\n",
      " rewrite json/place/E07000029.json (94%)\n",
      " rewrite json/place/E07000030.json (95%)\n",
      " rewrite json/place/E07000031.json (95%)\n",
      " rewrite json/place/E07000032.json (95%)\n",
      " rewrite json/place/E07000033.json (95%)\n",
      " rewrite json/place/E07000034.json (95%)\n",
      " rewrite json/place/E07000035.json (95%)\n",
      " rewrite json/place/E07000036.json (95%)\n",
      " rewrite json/place/E07000037.json (95%)\n",
      " rewrite json/place/E07000038.json (95%)\n",
      " rewrite json/place/E07000039.json (94%)\n",
      " rewrite json/place/E07000040.json (95%)\n",
      " rewrite json/place/E07000041.json (94%)\n",
      " rewrite json/place/E07000042.json (95%)\n",
      " rewrite json/place/E07000043.json (95%)\n",
      " rewrite json/place/E07000044.json (94%)\n",
      " rewrite json/place/E07000045.json (95%)\n",
      " rewrite json/place/E07000046.json (95%)\n",
      " rewrite json/place/E07000047.json (95%)\n",
      " rewrite json/place/E07000061.json (95%)\n",
      " rewrite json/place/E07000062.json (95%)\n",
      " rewrite json/place/E07000063.json (95%)\n",
      " rewrite json/place/E07000064.json (95%)\n",
      " rewrite json/place/E07000065.json (94%)\n",
      " rewrite json/place/E07000066.json (95%)\n",
      " rewrite json/place/E07000067.json (94%)\n",
      " rewrite json/place/E07000068.json (95%)\n",
      " rewrite json/place/E07000069.json (95%)\n",
      " rewrite json/place/E07000070.json (94%)\n",
      " rewrite json/place/E07000071.json (95%)\n",
      " rewrite json/place/E07000072.json (95%)\n",
      " rewrite json/place/E07000073.json (96%)\n",
      " rewrite json/place/E07000074.json (95%)\n",
      " rewrite json/place/E07000075.json (95%)\n",
      " rewrite json/place/E07000076.json (94%)\n",
      " rewrite json/place/E07000077.json (95%)\n",
      " rewrite json/place/E07000078.json (95%)\n",
      " rewrite json/place/E07000079.json (95%)\n",
      " rewrite json/place/E07000080.json (95%)\n",
      " rewrite json/place/E07000081.json (95%)\n",
      " rewrite json/place/E07000082.json (95%)\n",
      " rewrite json/place/E07000083.json (95%)\n",
      " rewrite json/place/E07000084.json (94%)\n",
      " rewrite json/place/E07000085.json (94%)\n",
      " rewrite json/place/E07000086.json (94%)\n",
      " rewrite json/place/E07000087.json (95%)\n",
      " rewrite json/place/E07000088.json (94%)\n",
      " rewrite json/place/E07000089.json (95%)\n",
      " rewrite json/place/E07000090.json (95%)\n",
      " rewrite json/place/E07000091.json (96%)\n",
      " rewrite json/place/E07000092.json (94%)\n",
      " rewrite json/place/E07000093.json (93%)\n",
      " rewrite json/place/E07000094.json (94%)\n",
      " rewrite json/place/E07000095.json (95%)\n",
      " rewrite json/place/E07000096.json (94%)\n",
      " rewrite json/place/E07000098.json (95%)\n",
      " rewrite json/place/E07000099.json (95%)\n",
      " rewrite json/place/E07000102.json (94%)\n",
      " rewrite json/place/E07000103.json (95%)\n",
      " rewrite json/place/E07000105.json (95%)\n",
      " rewrite json/place/E07000106.json (94%)\n",
      " rewrite json/place/E07000107.json (95%)\n",
      " rewrite json/place/E07000108.json (93%)\n",
      " rewrite json/place/E07000109.json (94%)\n",
      " rewrite json/place/E07000110.json (95%)\n",
      " rewrite json/place/E07000111.json (95%)\n",
      " rewrite json/place/E07000112.json (95%)\n",
      " rewrite json/place/E07000113.json (95%)\n",
      " rewrite json/place/E07000114.json (94%)\n",
      " rewrite json/place/E07000115.json (95%)\n",
      " rewrite json/place/E07000116.json (94%)\n",
      " rewrite json/place/E07000117.json (95%)\n",
      " rewrite json/place/E07000118.json (95%)\n",
      " rewrite json/place/E07000119.json (95%)\n",
      " rewrite json/place/E07000120.json (95%)\n",
      " rewrite json/place/E07000121.json (94%)\n",
      " rewrite json/place/E07000122.json (94%)\n",
      " rewrite json/place/E07000123.json (95%)\n",
      " rewrite json/place/E07000124.json (95%)\n",
      " rewrite json/place/E07000125.json (95%)\n",
      " rewrite json/place/E07000126.json (95%)\n",
      " rewrite json/place/E07000127.json (95%)\n",
      " rewrite json/place/E07000128.json (95%)\n",
      " rewrite json/place/E07000129.json (94%)\n",
      " rewrite json/place/E07000130.json (95%)\n",
      " rewrite json/place/E07000131.json (95%)\n",
      " rewrite json/place/E07000132.json (95%)\n",
      " rewrite json/place/E07000133.json (94%)\n",
      " rewrite json/place/E07000134.json (95%)\n",
      " rewrite json/place/E07000135.json (95%)\n",
      " rewrite json/place/E07000136.json (95%)\n",
      " rewrite json/place/E07000137.json (95%)\n",
      " rewrite json/place/E07000138.json (95%)\n",
      " rewrite json/place/E07000139.json (95%)\n",
      " rewrite json/place/E07000140.json (96%)\n",
      " rewrite json/place/E07000141.json (95%)\n",
      " rewrite json/place/E07000142.json (95%)\n",
      " rewrite json/place/E07000143.json (95%)\n",
      " rewrite json/place/E07000144.json (95%)\n",
      " rewrite json/place/E07000145.json (95%)\n",
      " rewrite json/place/E07000146.json (94%)\n",
      " rewrite json/place/E07000147.json (94%)\n",
      " rewrite json/place/E07000148.json (94%)\n",
      " rewrite json/place/E07000149.json (95%)\n",
      " rewrite json/place/E07000163.json (95%)\n",
      " rewrite json/place/E07000164.json (95%)\n",
      " rewrite json/place/E07000165.json (94%)\n",
      " rewrite json/place/E07000166.json (93%)\n",
      " rewrite json/place/E07000167.json (95%)\n",
      " rewrite json/place/E07000168.json (95%)\n",
      " rewrite json/place/E07000169.json (95%)\n",
      " rewrite json/place/E07000170.json (95%)\n",
      " rewrite json/place/E07000171.json (95%)\n",
      " rewrite json/place/E07000172.json (95%)\n",
      " rewrite json/place/E07000173.json (95%)\n",
      " rewrite json/place/E07000174.json (94%)\n",
      " rewrite json/place/E07000175.json (95%)\n",
      " rewrite json/place/E07000176.json (94%)\n",
      " rewrite json/place/E07000177.json (95%)\n",
      " rewrite json/place/E07000178.json (95%)\n",
      " rewrite json/place/E07000179.json (95%)\n",
      " rewrite json/place/E07000180.json (95%)\n",
      " rewrite json/place/E07000181.json (95%)\n",
      " rewrite json/place/E07000187.json (95%)\n",
      " rewrite json/place/E07000188.json (94%)\n",
      " rewrite json/place/E07000189.json (94%)\n",
      " rewrite json/place/E07000192.json (95%)\n",
      " rewrite json/place/E07000193.json (94%)\n",
      " rewrite json/place/E07000194.json (95%)\n",
      " rewrite json/place/E07000195.json (94%)\n",
      " rewrite json/place/E07000196.json (95%)\n",
      " rewrite json/place/E07000197.json (95%)\n",
      " rewrite json/place/E07000198.json (95%)\n",
      " rewrite json/place/E07000199.json (95%)\n",
      " rewrite json/place/E07000200.json (95%)\n",
      " rewrite json/place/E07000202.json (95%)\n",
      " rewrite json/place/E07000203.json (95%)\n",
      " rewrite json/place/E07000207.json (95%)\n",
      " rewrite json/place/E07000208.json (95%)\n",
      " rewrite json/place/E07000209.json (94%)\n",
      " rewrite json/place/E07000210.json (95%)\n",
      " rewrite json/place/E07000211.json (94%)\n",
      " rewrite json/place/E07000212.json (94%)\n",
      " rewrite json/place/E07000213.json (95%)\n",
      " rewrite json/place/E07000214.json (95%)\n",
      " rewrite json/place/E07000215.json (95%)\n",
      " rewrite json/place/E07000216.json (95%)\n",
      " rewrite json/place/E07000217.json (94%)\n",
      " rewrite json/place/E07000218.json (95%)\n",
      " rewrite json/place/E07000219.json (95%)\n",
      " rewrite json/place/E07000220.json (94%)\n",
      " rewrite json/place/E07000221.json (95%)\n",
      " rewrite json/place/E07000222.json (94%)\n",
      " rewrite json/place/E07000223.json (95%)\n",
      " rewrite json/place/E07000224.json (94%)\n",
      " rewrite json/place/E07000225.json (94%)\n",
      " rewrite json/place/E07000226.json (96%)\n",
      " rewrite json/place/E07000227.json (95%)\n",
      " rewrite json/place/E07000228.json (95%)\n",
      " rewrite json/place/E07000229.json (93%)\n",
      " rewrite json/place/E07000234.json (95%)\n",
      " rewrite json/place/E07000235.json (94%)\n",
      " rewrite json/place/E07000236.json (94%)\n",
      " rewrite json/place/E07000237.json (94%)\n",
      " rewrite json/place/E07000238.json (95%)\n",
      " rewrite json/place/E07000239.json (94%)\n",
      " rewrite json/place/E07000240.json (94%)\n",
      " rewrite json/place/E07000241.json (95%)\n",
      " rewrite json/place/E07000242.json (94%)\n",
      " rewrite json/place/E07000243.json (94%)\n",
      " rewrite json/place/E07000244.json (95%)\n",
      " rewrite json/place/E07000245.json (95%)\n",
      " rewrite json/place/E07000246.json (95%)\n",
      " rewrite json/place/E08000001.json (93%)\n",
      " rewrite json/place/E08000002.json (94%)\n",
      " rewrite json/place/E08000003.json (94%)\n",
      " rewrite json/place/E08000004.json (94%)\n",
      " rewrite json/place/E08000005.json (93%)\n",
      " rewrite json/place/E08000006.json (94%)\n",
      " rewrite json/place/E08000007.json (94%)\n",
      " rewrite json/place/E08000008.json (94%)\n",
      " rewrite json/place/E08000009.json (94%)\n",
      " rewrite json/place/E08000010.json (94%)\n",
      " rewrite json/place/E08000011.json (94%)\n",
      " rewrite json/place/E08000012.json (93%)\n",
      " rewrite json/place/E08000013.json (92%)\n",
      " rewrite json/place/E08000014.json (93%)\n",
      " rewrite json/place/E08000015.json (94%)\n",
      " rewrite json/place/E08000016.json (94%)\n",
      " rewrite json/place/E08000017.json (93%)\n",
      " rewrite json/place/E08000018.json (93%)\n",
      " rewrite json/place/E08000019.json (93%)\n",
      " rewrite json/place/E08000021.json (94%)\n",
      " rewrite json/place/E08000022.json (93%)\n",
      " rewrite json/place/E08000023.json (95%)\n",
      " rewrite json/place/E08000024.json (94%)\n",
      " rewrite json/place/E08000025.json (91%)\n",
      " rewrite json/place/E08000026.json (94%)\n",
      " rewrite json/place/E08000027.json (93%)\n",
      " rewrite json/place/E08000028.json (93%)\n",
      " rewrite json/place/E08000029.json (94%)\n",
      " rewrite json/place/E08000030.json (94%)\n",
      " rewrite json/place/E08000031.json (94%)\n",
      " rewrite json/place/E08000032.json (92%)\n",
      " rewrite json/place/E08000033.json (95%)\n",
      " rewrite json/place/E08000034.json (92%)\n",
      " rewrite json/place/E08000035.json (93%)\n",
      " rewrite json/place/E08000036.json (94%)\n",
      " rewrite json/place/E08000037.json (93%)\n",
      " rewrite json/place/E09000001.json (93%)\n",
      " rewrite json/place/E09000002.json (92%)\n",
      " rewrite json/place/E09000003.json (94%)\n",
      " rewrite json/place/E09000004.json (94%)\n",
      " rewrite json/place/E09000005.json (93%)\n",
      " rewrite json/place/E09000006.json (94%)\n",
      " rewrite json/place/E09000007.json (94%)\n",
      " rewrite json/place/E09000008.json (94%)\n",
      " rewrite json/place/E09000009.json (91%)\n",
      " rewrite json/place/E09000010.json (92%)\n",
      " rewrite json/place/E09000011.json (92%)\n",
      " rewrite json/place/E09000012.json (94%)\n",
      " rewrite json/place/E09000013.json (94%)\n",
      " rewrite json/place/E09000014.json (93%)\n",
      " rewrite json/place/E09000015.json (94%)\n",
      " rewrite json/place/E09000016.json (94%)\n",
      " rewrite json/place/E09000017.json (85%)\n",
      " rewrite json/place/E09000018.json (95%)\n",
      " rewrite json/place/E09000019.json (94%)\n",
      " rewrite json/place/E09000020.json (95%)\n",
      " rewrite json/place/E09000021.json (94%)\n",
      " rewrite json/place/E09000022.json (92%)\n",
      " rewrite json/place/E09000023.json (94%)\n",
      " rewrite json/place/E09000024.json (94%)\n",
      " rewrite json/place/E09000025.json (93%)\n",
      " rewrite json/place/E09000026.json (94%)\n",
      " rewrite json/place/E09000027.json (95%)\n",
      " rewrite json/place/E09000028.json (93%)\n",
      " rewrite json/place/E09000029.json (95%)\n",
      " rewrite json/place/E09000030.json (93%)\n",
      " rewrite json/place/E09000031.json (94%)\n",
      " rewrite json/place/E09000032.json (88%)\n",
      " rewrite json/place/E09000033.json (95%)\n",
      " rewrite json/place/W06000001.json (91%)\n",
      " rewrite json/place/W06000002.json (94%)\n",
      " rewrite json/place/W06000003.json (92%)\n",
      " rewrite json/place/W06000004.json (93%)\n",
      " rewrite json/place/W06000005.json (92%)\n",
      " rewrite json/place/W06000006.json (95%)\n",
      " rewrite json/place/W06000008.json (92%)\n",
      " rewrite json/place/W06000009.json (93%)\n",
      " rewrite json/place/W06000010.json (94%)\n",
      " rewrite json/place/W06000011.json (93%)\n",
      " rewrite json/place/W06000012.json (95%)\n",
      " rewrite json/place/W06000013.json (95%)\n",
      " rewrite json/place/W06000014.json (95%)\n",
      " rewrite json/place/W06000015.json (93%)\n",
      " rewrite json/place/W06000016.json (92%)\n",
      " rewrite json/place/W06000018.json (93%)\n",
      " rewrite json/place/W06000019.json (95%)\n",
      " rewrite json/place/W06000020.json (93%)\n",
      " rewrite json/place/W06000021.json (94%)\n",
      " rewrite json/place/W06000022.json (94%)\n",
      " rewrite json/place/W06000023.json (94%)\n",
      " rewrite json/place/W06000024.json (93%)\n",
      " create mode 100644 laddata.csv\n",
      "Enumerating objects: 573, done.\n",
      "Counting objects: 100% (573/573), done.\n",
      "Delta compression using up to 16 threads\n",
      "Compressing objects: 100% (335/335), done.\n",
      "Writing objects: 100% (335/335), 6.27 MiB | 839.00 KiB/s, done.\n",
      "Total 335 (delta 332), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (332/332), completed with 224 local objects.\u001b[K\n",
      "To https://github.com/theojolliffe/census-data.git\n",
      "   e5f858bc..e0defab4  main -> main\n",
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Add files\"\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fa7e1d7d-1357-4e65-a319-6d9441b66fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lads[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "381b23aa-2a04-4eb3-be22-56d3ce86bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lad</th>\n",
       "      <th>2001</th>\n",
       "      <th>2011</th>\n",
       "      <th>change</th>\n",
       "      <th>natRank</th>\n",
       "      <th>localRank</th>\n",
       "      <th>topic</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E06000001</th>\n",
       "      <td>Hartlepool</td>\n",
       "      <td>88611.0</td>\n",
       "      <td>92028.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-73</td>\n",
       "      <td>-6</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000002</th>\n",
       "      <td>Middlesbrough</td>\n",
       "      <td>134855.0</td>\n",
       "      <td>138412.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-43</td>\n",
       "      <td>-4</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000003</th>\n",
       "      <td>Redcar and Cleveland</td>\n",
       "      <td>139132.0</td>\n",
       "      <td>135177.0</td>\n",
       "      <td>-2.84</td>\n",
       "      <td>-5</td>\n",
       "      <td>-2</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000004</th>\n",
       "      <td>Stockton-on-Tees</td>\n",
       "      <td>178408.0</td>\n",
       "      <td>191610.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000005</th>\n",
       "      <td>Darlington</td>\n",
       "      <td>97838.0</td>\n",
       "      <td>105564.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>population_all</td>\n",
       "      <td>North East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000020</th>\n",
       "      <td>Torfaen</td>\n",
       "      <td>4.93</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000021</th>\n",
       "      <td>Monmouthshire</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.04</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000022</th>\n",
       "      <td>Newport</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1.06</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000023</th>\n",
       "      <td>Powys</td>\n",
       "      <td>16.81</td>\n",
       "      <td>17.39</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W06000024</th>\n",
       "      <td>Merthyr Tydfil</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.74</td>\n",
       "      <td>1.47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>economic_self-employed</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14147 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            lad      2001      2011 change natRank localRank  \\\n",
       "E06000001            Hartlepool   88611.0   92028.0   3.86     -73        -6   \n",
       "E06000002         Middlesbrough  134855.0  138412.0   2.64     -43        -4   \n",
       "E06000003  Redcar and Cleveland  139132.0  135177.0  -2.84      -5        -2   \n",
       "E06000004      Stockton-on-Tees  178408.0  191610.0    7.4     128         3   \n",
       "E06000005            Darlington   97838.0  105564.0    7.9     118         2   \n",
       "...                         ...       ...       ...    ...     ...       ...   \n",
       "W06000020               Torfaen      4.93      5.83    0.9     -10       -10   \n",
       "W06000021         Monmouthshire     10.49     12.04   1.55       1         1   \n",
       "W06000022               Newport       5.5      6.56   1.06       9         9   \n",
       "W06000023                 Powys     16.81     17.39   0.58      -4        -4   \n",
       "W06000024        Merthyr Tydfil      4.27      5.74   1.47       2         2   \n",
       "\n",
       "                            topic      parent  \n",
       "E06000001          population_all  North East  \n",
       "E06000002          population_all  North East  \n",
       "E06000003          population_all  North East  \n",
       "E06000004          population_all  North East  \n",
       "E06000005          population_all  North East  \n",
       "...                           ...         ...  \n",
       "W06000020  economic_self-employed       Wales  \n",
       "W06000021  economic_self-employed       Wales  \n",
       "W06000022  economic_self-employed       Wales  \n",
       "W06000023  economic_self-employed       Wales  \n",
       "W06000024  economic_self-employed       Wales  \n",
       "\n",
       "[14147 rows x 8 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95365f30-6d76-43a0-9192-3b6861621d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
